# 1. Transformer 背景介绍

[toc]

## 1.1 Transformer 的诞生

2018 年 10 月, Google 发出一篇论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》, BERT 模型横空出世, 并横扫 NLP 领域 11 项任务的最佳成绩!

论文地址: https://arxiv.org/pdf/1810.04805.pdf

而在 BERT 中发挥重要作用的结构就是 Transformer, 之后又相继出现 XLNET, roBERT 等模型击败了 BERT, 但是他们的核心没有变, 仍然是: Transformer.

## 1.2 Transformer 的优势

相比之前占领市场的 LSTM 和 GRU 模型, Transformer 有两个显著的优势:

1. Transformer能够利用分布式GPU进行并行训练, 提升模型训练效率.
2. 在分析预测更长的文本时, 捕捉间隔较长的语义关联效果更好.   

下面是一张在测评比较图:

<img src="https://sliu.vip/machine-learning/transformer-bg/2.png" alt="img" style="zoom: 67%;" />

## 1.3 Transformer 的市场

在著名的 SOTA 机器翻译榜单上, 几乎所有排名靠前的模型都使用 Transformer,

![img](https://sliu.vip/machine-learning/transformer-bg/3.png)

其基本上可以看作是工业界的风向标, 市场空间自然不必多说！