# 2. Transformer 架构解析

[toc]

## 2.1 认识 Transformer 架构

- Transformer 模型的作用

    基于 seq2seq 架构的 transformer 模型可以完成 NLP 领域研究的典型任务, 如机器翻译, 文本生成等. 同时又可以构建预训练语言模型, 用于不同任务的迁移学习.

> 声明: 在接下来的架构分析中, 我们将假设使用 Transformer 模型架构处理从一种语言文本到另一种语言文本的翻译工作, 因此很多命名方式遵循 NLP 中的规则. 比如: Embeddding 层将称作文本嵌入层, Embedding 层产生的张量称为词嵌入张量, 它的最后一维将称作词向量等.

- Transformer 总体架构图

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/4.png" alt="img" style="zoom: 67%;" />

- Transformer 总体架构可分为四个部分:
    - 输入部分
    - 输出部分
    - 编码器部分
    - 解码器部分

1. 输入部分包含:

- 源文本嵌入层及其位置编码器
- 目标文本嵌入层及其位置编码器

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/5.png" alt="img" style="zoom:67%;" />

2. 输出部分包含:

- 线性层
- softmax 层

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/6.png" alt="img" style="zoom:67%;" />

3. 编码器部分:

- 由 N 个编码器层堆叠而成
- 每个编码器层由两个子层连接结构组成
- 第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接
- 第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/7.png" alt="img" style="zoom:67%;" />

4. 解码器部分:

- 由 N 个解码器层堆叠而成
- 每个解码器层由三个子层连接结构组成
- 第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接
- 第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接
- 第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/8.png" alt="img" style="zoom:67%;" />

## 2.2 输入部分实现

- 输入部分包含:
    - 源文本嵌入层及其位置编码器
    - 目标文本嵌入层及其位置编码器

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/5.png" alt="img" style="zoom:67%;" />

- 文本嵌入层的作用

    无论是源文本嵌入还是目标文本嵌入, 都是为了将文本中词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.

- `nn.Embedding` 演示:

```python
embedding = nn.Embedding(10, 3)
x = torch.LongTensor([[1,2,4,5],[4,3,2,9]])
print(embedding(x))

embedding = nn.Embedding(10, 3, padding_idx=0)
x = torch.LongTensor([[0,2,0,5]])
print(embedding(x))
```

```python
tensor([[[ 0.8535, -1.0569, -0.4694],
         [ 0.0853, -0.3194, -0.7089],
         [ 0.0350,  2.3231,  0.2975],
         [-0.5877,  0.6385, -0.1938]],

        [[ 0.0350,  2.3231,  0.2975],
         [-0.7256, -0.3354, -0.7869],
         [ 0.0853, -0.3194, -0.7089],
         [ 0.5815, -0.7144,  0.3608]]], grad_fn=<EmbeddingBackward>)
tensor([[[ 0.0000,  0.0000,  0.0000],
         [-0.5823, -1.1718, -0.4988],
         [ 0.0000,  0.0000,  0.0000],
         [-1.2272,  0.7890,  0.4873]]], grad_fn=<EmbeddingBackward>)
```

- 文本嵌入层的代码分析:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
# torch中变量封装函数Variable.
from torch.autograd import Variable
import math
import matplotlib.pyplot as plt
import numpy as np
import copy


# 定义Embeddings类来实现文本嵌入层, 这里s说明代表两个一模一样的嵌入层, 他们共享参数.
class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        """
        d_model:	词嵌入的维度
        vocab:		指词表的大小
        """
        # 接着就是使用super的方式指明继承nn.Module的初始化函数
        super(Embeddings, self).__init__()
        # 定义Embedding层
        self.lut = nn.Embedding(vocab, d_model)
        # 将参数传入类中
        self.d_model = d_model

    def forward(self, x):
        """
        当传给该类的实例化对象参数时, 自动调用该类函数
        x: 代表输入给模型的文本通过词汇映射后的张量
        """
        return self.lut(x) * math.sqrt(self.d_model)
```

- 调用:

```python
# 词嵌入维度是512维
d_model = 512

# 词表大小是1000
vocab = 1000

# 输入x是一个使用Variable封装的长整型张量, 形状是2 x 4
x = Variable(torch.LongTensor([[100, 2, 421, 508],[491, 998, 1, 221]]))

emb = Embeddings(d_model, vocab)
embr = emb(x)
print("embr:", embr)
# 2*4*512
print(embr.shape)
```

- 输出效果:

```python
embr: tensor([[[  4.7566,   8.0452, -14.3616,  ...,  12.0641, -16.5267,  15.6591],
         [ 10.1216, -28.4124,  -3.3370,  ...,  15.4527,  22.2002,  19.7194],
         [ -8.2468,  15.5168,  25.9405,  ..., -18.6763,   5.7895, -24.9611],
         [-14.6167,  -3.4341,  39.4731,  ...,  20.2790, -12.5186,  32.8708]],

        [[ 17.3317,  10.2113,  -6.5570,  ...,   3.4141, -22.1003,   7.7803],
         [ 29.0737,  22.3649,  -9.4444,  ...,   1.2338, -29.9390,   3.4128],
         [ 10.4743,  12.4815,  10.8294,  ..., -10.2896, -11.7144,  -4.7382],
         [-41.0946, -15.1964, -18.2530,  ...,   4.6479, -19.1420,  42.0400]]],
       grad_fn=<MulBackward0>)
torch.Size([2, 4, 512])
```

- 位置编码器的作用

    因为在 Transformer 的编码器结构中, 并没有针对词汇位置信息的处理, 因此需要在 Embedding 层后加入位置编码器, 将词汇位置不同可能会产生不同语义的信息加入到词嵌入张量中, 以弥补位置信息的缺失.

- `nn.Dropout` 演示:

```python
m = nn.Dropout(p=0.2)
x = torch.randn(4, 5)
output = m(x)
print(output)
```

```python
tensor([[-0.0000,  1.5197,  1.3591,  1.2780,  0.6114],
        [-1.0075, -1.1205, -0.0000,  2.6063, -1.9405],
        [-1.7597, -1.6342, -0.4157,  0.0000, -0.6939],
        [-1.2565, -1.4085,  1.1746, -0.0000, -3.8221]])
```

- 位置编码器的代码分析:

```python
# 构建位置编码器类    
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):
        """
        d_model: 词嵌入维度, 
        dropout: 置0比率
        max_len: 每个句子的最大长度
        """
        super(PositionalEncoding, self).__init__()

        self.dropout = nn.Dropout(p=dropout)

        # 初始化位置编码矩阵
        pe = torch.zeros(max_len, d_model)

        # 初始化绝对位置矩阵, 用它的索引去表示. 首先使用arange方法获得一个连续自然数向量, 然后再使用unsqueeze方法拓展向量维度使其成为 max_len*1 的矩阵,    
        position = torch.arange(0, max_len).unsqueeze(1)

        # 绝对位置矩阵初始化之后, 接下来就是考虑如何将这些位置信息加入到位置编码矩阵中, 
        # 最简单思路就是先将 max_len*1 的绝对位置矩阵, 变换成 max_len*d_model 形状, 然后覆盖原来的初始位置编码矩阵即可
        # 要做这种矩阵变换, 就需要一个 1*d_model 形状的变换矩阵div_term, 我们对这个变换矩阵的要求除了形状外
        # 还希望它能够将自然数的绝对位置编码缩放成足够小的数字, 有助于在之后的梯度下降过程中更快的收敛
        # 首先使用arange获得一个自然数矩阵,  但是细心的同学们会发现,  我们这里并没有按照预计的一样初始化一个 1*d_model 的矩阵
        # 而是有了一个跳跃, 只初始化了一半即 1*d_model/2 的矩阵. 其实这里并不是真正意义上的初始化了一半的矩阵
        # 我们可以把它看作是初始化了两次, 而每次初始化的变换矩阵会做不同的处理, 第一次初始化的变换矩阵分布在正弦波上, 第二次初始化的变换矩阵分布在余弦波上
        # 并把这两个矩阵分别填充在位置编码矩阵的偶数和奇数位置上, 组成最终的位置编码矩阵
        div_term = torch.exp(torch.arange(0, d_model, 2) *
                             -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        # 这样我们就得到了位置编码矩阵pe, pe现在还只是一个二维矩阵, 要想和embedding的输出 (一个三维张量) 相加, 就必须拓展一个维度
        pe = pe.unsqueeze(0)

        # 最后把pe位置编码矩阵注册成模型的buffer, 什么是buffer呢, 
        # 我们把它认为是对模型效果有帮助的, 但是却不是模型结构中超参数或者参数, 不需要随着优化步骤进行更新的增益对象.
        # 注册之后我们就可以在模型保存后重加载时和模型结构与参数一同被加载.
        self.register_buffer('pe', pe)

    def forward(self, x):
        """x: 表示文本序列的词嵌入表示"""
        # 在相加之前我们对pe做一些适配工作, 将这个三维张量的第二维也就是句子最大长度的那一维将切片到与输入的x的第二维相同即x.size(1), 
        # 因为我们默认max_len为5000一般来讲实在太大了, 很难有一条句子包含5000个词汇, 所以要进行与输入张量的适配. 
        # 最后使用Variable进行封装, 使其与x的样式相同, 但是它是不需要进行梯度求解的, 因此把requires_grad设置成false.
        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)
        # 最后使用self.dropout对象进行'丢弃'操作, 并返回结果.
        return self.dropout(x)
```

- 调用:

```python
d_model = 512
dropout = 0.1
max_len=60

# 输入x是Embedding层的输出的张量, 形状是 2*4*512
x = embr

pe = PositionalEncoding(d_model, dropout, max_len)
pe_result = pe(x)
print("pe_result:", pe_result)
print(pe_result.shape)
```

- 输出效果:

```python
pe_result: tensor([[[-20.0003,   6.1901,   4.6575,  ...,   8.8488,  47.3368,   8.9150],
         [  0.0000, -10.4186,  18.3753,  ...,  -3.7828,   2.0080,  -5.8281],
         [ -2.1976,  -0.0000,  17.0661,  ...,  -4.9118, -85.2947,   0.0000],
         [ -1.2836,  15.6713, -13.6180,  ..., -25.9630,  -0.0000,   3.4794]],

        [[-48.2920,  12.3873, -12.5504,  ...,  18.7025,   0.0000,   0.5945],
         [ 18.3421,  44.0482,  -0.3876,  ...,   6.8114,  -0.0000,  -9.7884],
         [-29.7437, -42.3504, -40.9557,  ...,  -9.0282,  -7.1684,  -0.0000],
         [-34.1890,   0.0000,  30.9612,  ...,  45.3069,  -0.0000,  -0.0000]]],
       grad_fn=<MulBackward0>)
torch.Size([2, 4, 512])
```

- 绘制词汇向量中特征的分布曲线:

```python
import matplotlib.pyplot as plt

# 创建一张15 x 5大小的画布
plt.figure(figsize=(15, 5))

# 实例化PositionalEncoding类对象, 词嵌入维度为20, 置零比率设置为0
pe = PositionalEncoding(20, 0)

# 然后向pe传入被Variable封装的tensor, 这样pe会直接执行forward函数, 
# 且这个tensor里的数值都是0, 被处理后相当于位置编码张量
y = pe(Variable(torch.zeros(1, 100, 20)))

# 然后定义画布的横纵坐标, 横坐标到100的长度, 纵坐标是某一个词汇中的某维特征在不同长度下对应的值
# 因为总共有20维之多, 我们这里只查看4, 5, 6, 7维的值.
plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())

# 在画布上填写维度提示信息
plt.legend(["dim %d"%p for p in [4, 5, 6, 7]])
plt.show()
```

- 输出效果:

![img](2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/Figure_1.png)

- 效果分析:

    - 每条颜色的曲线代表某一个词汇中的特征在不同位置的含义.

    - 保证同一词汇随着所在位置不同它对应位置嵌入向量会发生变化.

    - 正弦波和余弦波的值域范围都是 1 到 - 1 这又很好的控制了嵌入数值的大小, 有助于梯度的快速计算.

## 2.3 编码器部分实现

- 编码器部分:
    - 由 N 个编码器层堆叠而成
    - 每个编码器层由两个子层连接结构组成
    - 第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接
    - 第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/7.png" alt="img" style="zoom:67%;" />

### 2.3.1 掩码张量

- 什么是掩码张量:

    掩代表遮掩, 码就是我们张量中的数值, 它的尺寸不定, 里面一般只有 1 和 0 的元素, 代表位置被遮掩或者不被遮掩, 至于是 0 位置被遮掩还是 1 位置被遮掩可以自定义, 因此它的作用就是让另外一个张量中的一些数值被遮掩, 也可以说被替换, 它的表现形式是一个张量.

- 掩码张量的作用:

    在 transformer 中, 掩码张量的主要作用在应用 attention (将在下一小节讲解) 时, 有一些生成的 attention 张量中的值计算有可能已知了未来信息而得到的, 未来信息被看到是因为训练时会把整个输出结果都一次性进行 Embedding, 但是理论上解码器的的输出却不是一次就能产生最终结果的, 而是一次次通过上一次结果综合得出的, 因此, 未来的信息可能被提前利用, 所以, 我们会进行遮掩. 关于解码器的有关知识将在后面的章节中讲解.

- `np.triu` 演示:

```python
np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], k=-1)
np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], k=0)
np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], k=1)
```

```python
array([[ 1,  2,  3],
       [ 4,  5,  6],
       [ 0,  8,  9],
       [ 0,  0, 12]])
array([[1, 2, 3],
       [0, 5, 6],
       [0, 0, 9],
       [0, 0, 0]])
array([[0, 2, 3],
       [0, 0, 6],
       [0, 0, 0],
       [0, 0, 0]])
```

- 生成掩码张量的代码分析:

```python
def subsequent_mask(size):
    """
    生成向后遮掩的掩码张量
    size: 掩码张量最后两个维度, 形成一个方阵
    """
    attn_shape = (1, size, size)

    # 使用np.ones()先构建全1张量, 然后利用np.triu()形成上三角矩阵, 最后为了节约空间, 
    # 使其中的数据类型变为无符号8位整形unit8 
    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')

    # 反转三角矩阵
    return torch.from_numpy(1 - subsequent_mask)
```

- 调用:

```python
size = 5
sm = subsequent_mask(size)
print("sm:", sm)
```

- 输出效果:

```python
sm: tensor([[[1, 0, 0, 0, 0],
         [1, 1, 0, 0, 0],
         [1, 1, 1, 0, 0],
         [1, 1, 1, 1, 0],
         [1, 1, 1, 1, 1]]], dtype=torch.uint8)
```

- 掩码张量的可视化:

```python
plt.figure(figsize=(5,5))
plt.imshow(subsequent_mask(20)[0])
plt.show()
```

- 输出效果:

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/12.png" alt="img" style="zoom:67%;" />

- 效果分析:

    - 通过观察可视化方阵, 黄色是 1 的部分, 这里代表被遮掩, 紫色代表没有被遮掩的信息, 横坐标代表目标词汇的位置, 纵坐标代表可查看的位置；

    - 我们看到, 在 0 的位置我们一看望过去都是黄色的, 都被遮住了, 1 的位置一眼望过去还是黄色, 说明第一次词还没有产生, 从第二个位置看过去, 就能看到位置 1 的词, 其他位置看不到, 以此类推.

### 2.3.2 注意力机制

- 什么是注意力:

    我们观察事物时, 之所以能够快速判断一种事物 (当然允许判断是错误的), 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断, 而并非是从头到尾的观察一遍事物后, 才能有判断结果. 正是基于这样的理论, 就产生了注意力机制.

- 什么是注意力计算规则:

    它需要三个指定的输入 $Q (query), K (key), V (value)$, 然后通过公式得到注意力的计算结果, 这个结果代表 $query$ 在 $key$ 和 $value$ 作用下的表示. 而这个具体的计算规则有很多种, 我这里只介绍我们用到的这一种.

- 我们这里使用的注意力的计算规则:

$$
Attention(Q,K,V) = Softmax(\frac{Q \cdot K^T}{\sqrt{d_k}}) \cdot V
$$

- $Q, K, V$ 的比喻解释:

    假如我们有一个问题: 给出一段文本, 使用一些关键词对它进行描述! 为了方便统一正确答案, 这道题可能预先已经给大家写出了一些关键词作为提示.

    其中这些给出的提示就可以看作是 $key$, 而整个的文本信息就相当于是 $query$, $value$ 的含义则更抽象, 可以比作是你看到这段文本信息后, 脑子里浮现的答案信息, 
    这里我们又假设大家最开始都不是很聪明, 第一次看到这段文本后脑子里基本上浮现的信息就只有提示这些信息, 
    因此 $key$ 与 $value$ 基本是相同的, 但是随着我们对这个问题的深入理解, 通过我们的思考脑子里想起来的东西原来越多, 
    并且能够开始对我们 $query$ 也就是这段文本, 提取关键信息进行表示.  这就是注意力作用的过程,  通过这个过程, 
    我们最终脑子里的 $value$ 发生了变化, 
    根据提示 $key$ 生成了 $query$ 的关键词表示方法, 也就是另外一种特征表示方法.

    刚刚我们说到 $key$ 和 $value$ 一般情况下默认是相同, 与 $query$ 是不同的, 这种是我们一般的注意力输入形式, 
    但有一种特殊情况, 就是我们 $query$ 与 $key$ 和 $value$ 相同, 这种情况我们称为自注意力机制.

    就如同我们的刚刚的例子,  使用一般注意力机制, 是使用不同于给定文本的关键词表示它. 而自注意力机制,
    需要用给定文本自身来表达自己, 也就是说你需要从给定文本中抽取关键词来表述它, 相当于对文本自身的一次特征提取.

- 什么是注意力机制:

    注意力机制是注意力计算规则能够应用的深度学习网络的载体, 除了注意力计算规则外, 还包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体. 使用自注意力计算规则的注意力机制称为自注意力机制.

- 注意力机制在网络中实现的图形表示:

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/10.png" alt="img" style="zoom:67%;" />

- `tensor.masked_fill` 演示:

```python
x = Variable(torch.randn(5, 5))
print(x)

mask = Variable(torch.zeros(5, 5))
print(mask)

y = x.masked_fill(mask == 0, -1e9)
print(y)
```

```python
tensor([[ 1.2246,  0.7315, -0.5963,  1.3028,  0.4139],
        [-0.7911, -1.4162,  1.1040, -1.2517, -2.1321],
        [-0.9252, -0.3397,  2.1925,  1.0727, -1.1403],
        [ 1.1745, -0.6409, -1.5606, -0.3699,  0.4194],
        [ 1.0304,  0.9876,  1.0243,  2.1947,  1.8046]])
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]])
tensor([[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],
        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],
        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],
        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],
        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]])
```

- 注意力计算规则的代码分析:

```python
def attention(query, key, value, mask=None, dropout=None):
    """注意力机制的实现"""
    # 首先取query的最后一维大小, 代表词嵌入的维度
    d_k = query.size(-1)
    # 按照注意力公式, 将query与key的转置进行矩阵乘法, 然后除以缩放系数
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)

    # 接着判断是否使用掩码张量
    if mask is not None:
        # 利用tensor.masked_fill()方法, 将掩码张量和scores张量每个位置一一比较
        # 如果掩码张量处为0, 则对应的scores张量用-1e9这个值来替换
        scores = scores.masked_fill(mask==0, -1e9)

    # 对scores的最后一维进行softmax操作
    p_attn = F.softmax(scores, dim=-1)

    # 判断是否使用dropout
    if dropout is not None:
        p_attn = dropout(p_attn)

    # 返回query的注意力表示和注意力张量
    return torch.matmul(p_attn, value), p_attn
```

- 不带 mask 的调用:

```python
query = key = value = pe_result
attn, p_attn = attention(query, key, value)
print("attn:", attn)
print(attn.shape)
print("p_attn:", p_attn)
print(p_attn.shape)
```

- 不带 mask 的输出效果:

```python
# 将得到两个结果
# query的注意力表示:
attn: tensor([[[ 24.0071,  -9.1393, -37.2979,  ...,   0.3999, -23.7339,   0.0000],
         [ 13.9663, -35.7460,  12.4390,  ..., -19.3752,  28.5137, -35.6590],
         [-28.2353, -24.8867,  59.1204,  ..., -16.8469,   8.9484,  -1.8748],
         [  2.2843,   3.9388,  39.1144,  ...,  44.3272, -40.6454,   7.3852]],

        [[-29.2848, -30.3363,  26.0047,  ...,   0.0000,  -4.9971, -39.7574],
         [ -2.6293,  20.7029,  28.6776,  ..., -21.4901,  22.4527,  26.6080],
         [  9.8106, -35.6405,  15.1370,  ..., -23.9286,  36.8923,  51.0908],
         [  0.2826, -23.1399,  -2.4363,  ...,  -7.8705, -26.2878, -38.6924]]],
       grad_fn=<UnsafeViewBackward>)
torch.Size([2, 4, 512])
# 注意力张量:
p_attn: tensor([[[1., 0., 0., 0.],
         [0., 1., 0., 0.],
         [0., 0., 1., 0.],
         [0., 0., 0., 1.]],

        [[1., 0., 0., 0.],
         [0., 1., 0., 0.],
         [0., 0., 1., 0.],
         [0., 0., 0., 1.]]], grad_fn=<SoftmaxBackward>)
torch.Size([2, 4, 4])
```

- 带有 mask 的调用：

```python
query = key = value = pe_result
# 令mask为一个 2*4*4 的零张量
mask = Variable(torch.zeros(2, 4, 4))
attn, p_attn = attention(query, key, value, mask=mask)
print("attn:", attn)
print(attn.shape)
print("p_attn:", p_attn)
print(p_attn.shape)
```

- 带有 mask 的输出效果:

```python
# query的注意力表示:
attn: tensor([[[ -2.6259,  17.8723,  -1.0638,  ...,  17.0540, -12.8048, -15.1076],
         [ -2.6259,  17.8723,  -1.0638,  ...,  17.0540, -12.8048, -15.1076],
         [ -2.6259,  17.8723,  -1.0638,  ...,  17.0540, -12.8048, -15.1076],
         [ -2.6259,  17.8723,  -1.0638,  ...,  17.0540, -12.8048, -15.1076]],

        [[ -5.1133,  -0.0763,  13.1083,  ...,   3.3198, -10.6452,   5.6304],
         [ -5.1133,  -0.0763,  13.1083,  ...,   3.3198, -10.6452,   5.6304],
         [ -5.1133,  -0.0763,  13.1083,  ...,   3.3198, -10.6452,   5.6304],
         [ -5.1133,  -0.0763,  13.1083,  ...,   3.3198, -10.6452,   5.6304]]],
       grad_fn=<UnsafeViewBackward>)
torch.Size([2, 4, 512])
# 注意力张量:
p_attn: tensor([[[0.2500, 0.2500, 0.2500, 0.2500],
         [0.2500, 0.2500, 0.2500, 0.2500],
         [0.2500, 0.2500, 0.2500, 0.2500],
         [0.2500, 0.2500, 0.2500, 0.2500]],

        [[0.2500, 0.2500, 0.2500, 0.2500],
         [0.2500, 0.2500, 0.2500, 0.2500],
         [0.2500, 0.2500, 0.2500, 0.2500],
         [0.2500, 0.2500, 0.2500, 0.2500]]], grad_fn=<SoftmaxBackward>)
torch.Size([2, 4, 4])
```

### 2.3.3 多头注意力机制

- 什么是多头注意力机制:

    从多头注意力的结构图中, 貌似这个所谓的多个头就是指多组线性变换层, 其实并不是, 我只有使用了一组线性变化层, 即三个变换张量对 $Q, K, V$ 分别进行线性变换, 这些变换不会改变原有张量的尺寸, 因此每个变换矩阵都是方阵, 得到输出结果后, 多头的作用才开始显现, 每个头开始从词义层面分割输出的张量, 也就是每个头都想获得一组 $Q, K, V$ 进行注意力机制的计算, 但是句子中的每个词的表示只获得一部分, 也就是只分割了最后一维的词嵌入向量. 这就是所谓的多头, 将每个头的获得的输入送到注意力机制中, 就形成多头注意力机制.

- 多头注意力机制结构图:

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/13.png" alt="img" style="zoom: 50%;" />

- 多头注意力机制的作用:

    这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分, 从而均衡同一种注意力机制可能产生的偏差, 让词义拥有来自更多元的表达, 实验表明可以提升模型效果.

- tensor.view 演示:

```python
x = torch.randn(4, 4)
print(x.size())
y = x.view(16)
print(y.size())
z = x.view(-1, 8)  # the size -1 is inferred from other dimensions
print(z.size())

a = torch.randn(1, 2, 3, 4)
print(a.size())
print(a)

b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension
print(b.size())
print(b)

c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory
print(c.size())
print(c)

print(torch.equal(b, c))
```

```python
torch.Size([4, 4])
torch.Size([16])
torch.Size([2, 8])
torch.Size([1, 2, 3, 4])
tensor([[[[ 0.4151,  0.7801,  0.4805,  0.0283],
          [-0.3972,  1.6183,  0.5711,  0.7168],
          [-0.1170,  0.5736, -0.1436, -2.3956]],

         [[-1.1076,  0.8500, -0.3261, -0.8846],
          [-0.2708,  1.6612,  0.0776, -0.8178],
          [-0.5698, -0.8054,  0.5944, -1.2946]]]])
torch.Size([1, 3, 2, 4])
tensor([[[[ 0.4151,  0.7801,  0.4805,  0.0283],
          [-1.1076,  0.8500, -0.3261, -0.8846]],

         [[-0.3972,  1.6183,  0.5711,  0.7168],
          [-0.2708,  1.6612,  0.0776, -0.8178]],

         [[-0.1170,  0.5736, -0.1436, -2.3956],
          [-0.5698, -0.8054,  0.5944, -1.2946]]]])
torch.Size([1, 3, 2, 4])
tensor([[[[ 0.4151,  0.7801,  0.4805,  0.0283],
          [-0.3972,  1.6183,  0.5711,  0.7168]],

         [[-0.1170,  0.5736, -0.1436, -2.3956],
          [-1.1076,  0.8500, -0.3261, -0.8846]],

         [[-0.2708,  1.6612,  0.0776, -0.8178],
          [-0.5698, -0.8054,  0.5944, -1.2946]]]])
False
```

- 多头注意力机制的代码实现:

```python
# 用于深度拷贝的copy工具包
import copy

# 首先需要定义克隆函数, 因为在多头注意力机制的实现中, 用到多个结构相同的线性层.
# 我们将使用clone函数将他们一同初始化在一个网络层列表对象中. 之后的结构中也会用到该函数.
def clones(module, N):
    """
    用于生成相同网络层的克隆函数
    module: 要克隆的目标网络层
    N:		代表需要克隆的数量
    """
    # 通过for循环对module进行N次深度拷贝, 使其每个module成为独立的层
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])


class MultiHeadedAttention(nn.Module):
    def __init__(self, head, embedding_dim, dropout=0.1):
        """
        head: 			头数
        embedding_dim:	词嵌入的维度
        dropout:		进行dropout操作时置0比率, 默认是0.1
        """
        super(MultiHeadedAttention, self).__init__()
        # 需要确认一个事实, 多头的数量head能被词嵌入的维度embedding_dim整除
        assert embedding_dim % head == 0

        # 得到每个头获得的分割词向量维度d_k
        self.d_k = embedding_dim // head

        self.head = head
        self.embedding_dim = embedding_dim

        # 实例化4个线性层, 分别是Q, K, V以及最终的输出线性层
        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)

        # 初始化注意力张量, 现在还没有结果所以为None.
        self.attn = None

        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value, mask=None):
        # 判断是否使用掩码张量
        if mask is not None:
            # 使用unsqueeze拓展维度, 代表多头中的第n头
            mask = mask.unsqueeze(0)

        # 得到batch_size, 他是query尺寸的第1维大小, 代表有多少条样本
        batch_size = query.size(0)

        # 多头处理环节
        # 首先利用zip和for循环将线性层和三个输入QKV组到一起
        # 做完线性变换后, 开始为每个头分割输入. 使用view()方法对线性变换的结果进行维度重塑, 多加了一个维度h, 代表头数. 因此每个头可以获得一部分词特征组成的句子
        # 为了让代表句子长度维度和词向量维度能够相邻, 对第二维和第三维进行转置操作, 这样注意力机制才能找到词义与句子位置的关系, 
        # 从attention函数中可以看到, 利用的是原始输入的倒数第一和第二维
        query, key, value = [model(x).view(batch_size, -1, self.head, self.d_k).transpose(1, 2)
            for model, x in zip(self.linears, (query, key, value))]

        # 将每个头的输出传入attention层
        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)

        # 得到每个头计算结果组成的4维张量, 需要将其转换为输入的形状以方便后续的计算, 
        # 进行第一步处理环节的逆操作, 先对第2和第3维进行转置, 然后使用contiguous()方法, 
        # 注意: 经历了transpose()方法后, 必须使用contiguous()方法,不然无法使用view()方法
        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.head * self.d_k)

        # 最后将x输入线性层列表中的最后一个线性层中进行处理, 得到最终的多头注意力结构输出
        return self.linears[-1](x)
```

- 调用:

```python
head = 8
embedding_dim = 512
dropout = 0.2

# 假设输入的Q, K, V仍然相等, 2*4*512
query = value = key = pe_result
mask = Variable(torch.zeros(8, 4, 4))

mha = MultiHeadedAttention(head, embedding_dim, dropout)
mha_result = mha(query, key, value, mask)
print(mha_result)
# 2*4*512
print(mha_result.shape)
```

- 输出效果:

```python
tensor([[[ 1.3252, -2.1862, -0.7769,  ..., -2.8408, -4.2528, -4.5284],
         [-1.6535, -0.6782,  1.2259,  ..., -2.3344, -1.0284, -3.9207],
         [ 1.9194, -2.6968,  1.3048,  ...,  0.1957, -3.1415, -4.2354],
         [ 2.6284,  0.5297, -0.5216,  ...,  0.0923, -5.3603, -4.2055]],

        [[ 5.8704, -1.2624,  3.7162,  ...,  4.2234,  6.6254,  3.7720],
         [ 6.0680, -0.8771,  5.3790,  ...,  4.7585,  3.9130,  4.6115],
         [ 7.0520,  1.2335,  2.1412,  ...,  3.6339,  6.4847,  2.6429],
         [ 6.0067, -5.2073,  5.5901,  ...,  4.3745, -0.8325, -0.1963]]],
       grad_fn=<AddBackward0>)
torch.Size([2, 4, 512])
```

### 2.3.4 前馈全连接层

- 什么是前馈全连接层:

    在 Transformer 中前馈全连接层就是具有两层线性层的全连接网络.

- 前馈全连接层的作用:

    考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力.

- 前馈全连接层的代码分析:

```python
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        """
        d_model: 线性层的输入维度也是第二个线性层的输出维度 (希望维度不变)
        d_ff: 第二个线性层的输入维度和第一个线性层的输出维度
        dropout: 置0比率
        """
        super(PositionwiseFeedForward, self).__init__()

        self.w1 = nn.Linear(d_model, d_ff)
        self.w2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w2(self.dropout(F.relu(self.w1(x))))
```

- 调用:

```python
d_model = 512
d_ff = 64
dropout = 0.2

# 输入参数x可以是多头注意力机制的输出, 2*4*512
x = mha_result

ff = PositionwiseFeedForward(d_model, d_ff, dropout)
ff_result = ff(x)
print(ff_result)
# 2*4*512
print(ff_result.shape)
```

- 输出效果:

```python
tensor([[[-0.7433, -1.6425, -0.0078,  ...,  1.4373, -0.2675,  0.4839],
         [ 0.2781, -1.0085, -1.6424,  ...,  0.2158, -0.3293,  1.6110],
         [-0.9412, -0.0985, -0.9330,  ..., -0.5757,  0.7612,  1.3114],
         [ 1.2955, -1.4546, -0.1642,  ..., -0.2484, -0.1061,  1.0776]],

        [[-1.7605, -0.6515, -1.1349,  ..., -0.0778, -0.4827,  0.6523],
         [-0.8560,  0.1496,  0.7862,  ...,  2.2887, -0.3914,  0.6384],
         [-0.3229,  0.1979,  0.1962,  ...,  1.2020, -0.9368,  0.8709],
         [-1.4784,  0.3879,  0.7390,  ...,  1.2936,  0.0048,  2.1192]]],
       grad_fn=<AddBackward0>)
torch.Size([2, 4, 512])
```

ReLU 函数
$$
\text{ReLU}(x) = max(0, x)
$$
<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/ReLU.png" alt="ReLU" style="zoom: 33%;" />

### 2.3.5 规范化层

- 规范化层的作用:

    它是所有深层网络模型都需要的标准网络层, 因为随着网络层数的增加, 通过多层的计算后参数可能开始出现过大或过小的情况, 这样可能会导致学习过程出现异常, 模型可能收敛非常的慢。因此都会在一定层数后接规范化层进行数值的规范化, 使其特征数值在合理范围内.

- 规范化层的代码实现:

```python
# 通过LayerNorm实现规范化层的类
class LayerNorm(nn.Module):
    def __init__(self, features, eps=1e-6):
        """
        features: 词嵌入的维度,
        eps: 一个足够小的数, 在规范化公式的分母中出现, 防止分母为0. 默认是1e-6
        """
        super(LayerNorm, self).__init__()

        # 初始化两个参数张量a2, b2, 用于对结果做规范化操作计算 
        # 因为直接对上一层得到的结果做规范化公式计算, 将改变结果的正常表征
        # 因此就需要有参数作为调节因子, 使其即能满足规范化要求, 又能不改变针对目标的表征
        # 使用nn.Parameter()封装, 代表他们是模型的参数
        self.a2 = nn.Parameter(torch.ones(features))
        self.b2 = nn.Parameter(torch.zeros(features))

        self.eps = eps

    def forward(self, x):
        # 对x求其最后一个维度的均值和标准差, 并保持输出维度与输入维度一致
        # 用x减去均值除以标准差获得规范化的结果, 对结果乘以缩放参数a2, 加上位移参数b2
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a2 * (x - mean) / (std + self.eps) + self.b2
```

- 调用:

```python
features = d_model = 512
eps = 1e-6

# 输入x来自前馈全连接层的输出
x = ff_result

ln = LayerNorm(features, eps)
ln_result = ln(x)
print(ln_result)
# 2*4*512
print(ln_result.shape)
```

- 输出效果:

```python
tensor([[[-1.8784, -1.0511, -1.8118,  ...,  1.2466, -0.9167, -1.7972],
         [-1.5246, -0.5741, -1.5284,  ...,  0.7907, -0.9446, -0.0484],
         [-1.7641, -1.2820, -0.5942,  ..., -0.2100, -1.0638, -0.3859],
         [-1.7300, -0.4608, -1.1505,  ...,  0.8358, -1.9011, -0.6637]],

        [[-1.2626, -0.6530, -0.8197,  ...,  0.6901, -0.9263, -0.1679],
         [-2.1312, -0.7324, -0.6913,  ...,  1.4141, -0.9657, -0.4500],
         [-2.0524, -1.3971, -1.2146,  ...,  1.4342,  0.3321, -1.2669],
         [-1.0491, -0.1974, -0.7999,  ...,  1.2633, -1.6070, -1.5229]]],
       grad_fn=<AddBackward0>)
torch.Size([2, 4, 512])
```

### 2.3.6 子层连接结构

- 什么是子层连接结构:

    如图所示, 输入到每个子层以及规范化层的过程中, 还使用了残差链接 (跳跃连接), 因此我们把这一部分结构整体叫做子层连接 (代表子层及其链接结构), 在每个编码器层中, 都有两个子层, 这两个子层加上周围的链接结构就形成了两个子层连接结构.

- 子层连接结构图:

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/15.png" alt="img" style="zoom:50%;" />

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/16.png" alt="img" style="zoom:50%;" />

- 子层连接结构的代码分析:

```python
# 使用SublayerConnection来实现子层连接结构的类
class SublayerConnection(nn.Module):
    def __init__(self, size, dropout=0.1):
        """
        size:		词嵌入维度的大小
        dropout:	随机置0比率
        """
        super(SublayerConnection, self).__init__()

        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x, sublayer):
        """
        sublayer: 该子层连接中的子层函数
        """
        return x + self.dropout(sublayer(self.norm(x)))
```

- 调用:

```python
size = 512
dropout = 0.2
head = 8
d_model = 512

# 令x为位置编码器的输出
x = pe_result
mask = Variable(torch.zeros(8, 4, 4))

# 假设子层中装的是多头注意力层, 实例化这个类
self_attn =  MultiHeadedAttention(head, d_model)

# 使用lambda获得一个函数类型的子层
sublayer = lambda x: self_attn(x, x, x, mask)

sc = SublayerConnection(size, dropout)
sc_result = sc(x, sublayer)
print(sc_result)
print(sc_result.shape)
```

- 输出效果:

```python
tensor([[[ 23.0657,   9.4647,  57.3521,  ..., -33.8385, -14.2230,  -2.4823],
         [ 16.3373,  26.6969, -21.7211,  ...,  29.2276,  19.0764,  -3.3937],
         [ 21.1489, -30.9305, -11.8501,  ...,   5.3504,  35.3020,  53.5299],
         [-34.2603,  13.3675,  -8.1677,  ...,   0.2106,  -7.6713,  53.7182]],

        [[ 10.4260, -34.8735, -17.0119,  ..., -14.3394,  16.8712, -18.7870],
         [ 31.2219,   0.6739,  25.2412,  ...,   0.3759,  -1.2504,   2.0974],
         [ 12.5716,   3.2896,   7.5753,  ...,  45.6444,   6.4881, -21.1894],
         [ -9.4546, -16.8704,  -6.0934,  ..., -21.4988,   9.0759, -17.4816]]],
       grad_fn=<AddBackward0>)
torch.Size([2, 4, 512])
```

### 2.3.7 编码器层

- 编码器层的作用:

    作为编码器的组成单元, 每个编码器层完成一次对输入的特征提取过程, 即编码过程.

- 编码器层的构成图:

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/17.png" alt="img" style="zoom:50%;" />

- 编码器层的代码分析:

```python
class EncoderLayer(nn.Module):
    def __init__(self, size, self_attn, feed_forward, dropout):
        """
        size:			词嵌入维度的大小, 也即编码器层的大小, 
        self_attn:		多头自注意力子层实例化对象 
        feed_froward:	前馈全连接层实例化对象
        """
        super(EncoderLayer, self).__init__()

        self.self_attn = self_attn
        self.feed_forward = feed_forward
        # 编码器层中有两个子层连接结构, 使用clones函数克隆
        self.sublayer = clones(SublayerConnection(size, dropout), 2)
        self.size = size

    def forward(self, x, mask):
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
        return self.sublayer[1](x, self.feed_forward)
```

- 调用:

```python
size = 512
head = 8
d_model = 512
d_ff = 64
x = pe_result
dropout = 0.2

self_attn = MultiHeadedAttention(head, d_model)
ff = PositionwiseFeedForward(d_model, d_ff, dropout)
mask = Variable(torch.zeros(8, 4, 4))

el = EncoderLayer(size, self_attn, ff, dropout)
el_result = el(x, mask)
print(el_result)
# 2*4*512
print(el_result.shape)
```

- 输出效果:

```python
tensor([[[-11.2326, -23.8076, -17.7616,  ...,  -9.5839,  -0.3118,   0.1803],
         [  6.1065,  10.2600,   3.6833,  ...,  14.0489,  44.6775,   0.1886],
         [-29.1274, -16.5955, -15.0144,  ...,  -1.0211,  -7.8361, -36.2035],
         [ 51.4664, -20.2076,  -8.6730,  ...,  -0.3246, -63.5004,  39.0385]],

        [[ 24.4466,  16.1007, -38.8650,  ...,   1.8171, -45.4997,  18.8480],
         [ -0.1321, -30.2475, -36.2476,  ..., -36.1350,  25.4063,  23.1291],
         [ 23.8069,  -0.1674,  20.1947,  ..., -16.0422,   0.6919, -45.7318],
         [ -6.5457,   6.4547,   0.2463,  ...,   3.3944,  29.1932,  34.3736]]],
       grad_fn=<AddBackward0>)
torch.Size([2, 4, 512])
```

### 2.3.8 编码器

- 编码器的作用:

    编码器用于对输入进行指定的特征提取过程, 也称为编码, 由 N 个编码器层堆叠而成.

- 编码器的结构图:

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/7.png" alt="img" style="zoom:50%;" />

- 编码器的代码分析:

```python
class Encoder(nn.Module):
    def __init__(self, layer, N):
        """
        layer:	编码器层实例对象
        N:		编码器层个数
        """
        super(Encoder, self).__init__()
        # 首先使用clones()函数克隆N个编码器层放在self.layers中
        self.layers = clones(layer, N)
        # 再初始化一个规范化层, 它将用在编码器的最后面.
        self.norm = LayerNorm(layer.size)

    def forward(self, x, mask):
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
```

- 调用:

```python
size = 512
head = 8
d_model = 512
d_ff = 64
dropout = 0.2

attn = MultiHeadedAttention(head, d_model)
ff = PositionwiseFeedForward(d_model, d_ff, dropout)
# 因为编码器层中的子层是不共享的, 因此需要使用深度拷贝各个对象
c = copy.deepcopy
layer = EncoderLayer(size, c(attn), c(ff), dropout)

# 编码器中编码器层的个数N
N = 8
x = pe_result
mask = Variable(torch.zeros(8, 4, 4))

en = Encoder(layer, N)
en_result = en(x, mask)
print(en_result)
# 2*4*512
print(en_result.shape)
```

- 输出效果:

```python
tensor([[[-2.3456, -0.4438, -0.4616,  ...,  0.1620, -0.1137,  0.8467],
         [-0.0388,  0.2492, -2.7778,  ...,  0.4838,  1.7407, -1.1895],
         [-0.5399, -0.9112, -0.1751,  ..., -0.3876, -0.9585,  1.5904],
         [ 0.8109,  0.1148,  0.4642,  ..., -0.2097, -0.2373,  0.5680]],

        [[ 0.3091,  0.2768, -1.3355,  ..., -0.6018,  0.6683, -0.0445],
         [-0.6464,  2.6015,  0.4757,  ...,  0.7655,  0.5912,  0.3292],
         [ 0.3327, -1.9317, -1.4360,  ..., -1.8780, -0.8285, -1.1599],
         [ 0.4121, -0.1096,  0.5568,  ..., -2.3471, -0.7720, -0.9274]]],
       grad_fn=<AddBackward0>)
torch.Size([2, 4, 512])
```

## 2.4 解码器部分实现

- 解码器部分:
    - 由 N 个解码器层堆叠而成
    - 每个解码器层由三个子层连接结构组成
    - 第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接
    - 第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接
    - 第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/8.png" alt="img" style="zoom:67%;" />

> 解码器层中的各个部分, 如, 多头注意力机制, 规范化层, 前馈全连接网络, 子层连接结构都与编码器中的实现相同, 因此这里可以直接拿来构建解码器层.

### 2.4.1 解码器层

- 解码器层的作用:

    作为解码器的组成单元, 每个解码器层根据给定的输入向目标方向进行特征提取操作, 即解码过程.

- 解码器层的代码实现:

```python
class DecoderLayer(nn.Module):
    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        """
        size: 		词嵌入的维度大小, 也即码器层的尺寸
        self_attn:	多头自注意力对象 (Q=K=V) 
        src_attn:	多头注意力对象 (Q!=K=V)
        feed_forward: 前馈全连接层对象
        droupout: 	置0比率
        """
        super(DecoderLayer, self).__init__()

        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward

        # 克隆三个子层连接对象
        self.sublayer = clones(SublayerConnection(size, dropout), 3)

    def forward(self, x, memory, source_mask, target_mask):
        """
        x: 				上一层的输入张量
        mermory: 		编码器层的语义存储变量
        source_mask: 	源数据的掩码张量
        target_mask: 	目标数据的掩码张量
        """
        # 将memory表示成m方便之后使用
        m = memory

        # 第一个子层使用自注意力机制, Q=K=V=x 
        # target_mask对目标数据进行遮掩, 因为此时模型可能还没有生成任何目标数据
        # 比如在解码器准备生成第一个字符或词汇时, 我们其实已经传入了第一个字符以便计算损失
        # 但是我们不希望在生成第一个字符时模型能利用这个信息, 因此我们会将其遮掩, 同样生成第二个字符或词汇时
        # 模型只能使用第一个字符或词汇信息, 第二个字符以及之后的信息都不允许被模型使用
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, target_mask))

        # 第二个子层使用常规的注意力机制, Q=x, k=v=memory
        # source_mask对源数据进行遮掩, 原因并非是抑制信息泄漏
        # 而是遮蔽掉对结果没有意义的字符而产生的注意力值, 以此提升模型效果和训练速度.
        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, source_mask))

        # 最后为前馈全连接子层
        return self.sublayer[2](x, self.feed_forward)
```

- 调用:

```python
# 类的实例化参数与解码器层类似, 相比多出了src_attn, 但是和self_attn是同一个类.
head = 8
size = 512
d_model = 512
d_ff = 64
dropout = 0.2
self_attn = src_attn = MultiHeadedAttention(head, d_model, dropout)

# 前馈全连接层也和之前相同 
ff = PositionwiseFeedForward(d_model, d_ff, dropout)

# x是来自目标数据的词嵌入表示, 但形式和源数据的词嵌入表示相同
x = pe_result

# memory是来自编码器的输出
memory = en_result

# 实际中source_mask和target_mask并不相同, 为了方便计算令他们都为mask
mask = Variable(torch.zeros(8, 4, 4))
source_mask = target_mask = mask

dl = DecoderLayer(size, self_attn, src_attn, ff, dropout)
dl_result = dl(x, memory, source_mask, target_mask)
print(dl_result)
# 2*4*512
print(dl_result.shape)
```

- 输出效果:

```python
tensor([[[-31.5399,  -8.8026, -10.7423,  ...,  40.8246,   4.1629, -41.9796],
         [-15.2581,  40.4448, -29.8395,  ...,  -1.8821,  22.6100,  -0.4387],
         [ -2.4638,  21.0772,  -0.1675,  ..., -36.7340, -37.1736,   9.5633],
         [ 11.7464,  11.8723,  18.7187,  ..., -22.2269, -11.2871,  31.6881]],

        [[-10.4611,  31.9787, -14.8812,  ...,  -8.3858,  -0.4735,  11.7239],
         [  1.9393, -50.9248,  -6.2553,  ...,  39.4261, -11.0662,  32.4883],
         [ 19.3672,  -8.4104, -20.0550,  ...,   0.5879,   9.1683, -12.8780],
         [ -0.1037,   8.5183, -26.9404,  ..., -11.2149, -19.9363,   5.2284]]],
       grad_fn=<AddBackward0>)
torch.Size([2, 4, 512])
```

### 2.4.2 解码器

- 解码器的作用:

    根据编码器的结果以及上一次预测的结果, 对下一次可能出现的 '值' 进行特征表示.

- 解码器的代码分析:

```python
# 使用类Decoder来实现解码器
class Decoder(nn.Module):
    def __init__(self, layer, N):
        """
        layer:	解码器层实例化对象
        N:		解码器层的个数
        """
        super(Decoder, self).__init__()

        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, memory, source_mask, target_mask):
        """
        x: 				目标数据的嵌入表示
        memory: 		编码器层的输出,
        source_mask: 	代表源数据的掩码张量
        target_mask: 	目标数据的掩码张量
        """
        for layer in self.layers:
            x = layer(x, memory, source_mask, target_mask)
        return self.norm(x)
```

- 调用:

```python
size = 512
d_model = 512
head = 8
d_ff = 64
dropout = 0.2
c = copy.deepcopy
attn = MultiHeadedAttention(head, d_model)
ff = PositionwiseFeedForward(d_model, d_ff, dropout)
layer = DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout)
N = 8

x = pe_result
memory = en_result
mask = Variable(torch.zeros(8, 4, 4))
source_mask = target_mask = mask

de = Decoder(layer, N)
de_result = de(x, memory, source_mask, target_mask)
print(de_result)
# 2*4*512
print(de_result.shape)
```

- 输出效果:

```python
tensor([[[ 0.0347, -0.0984,  0.0673,  ...,  0.1903,  0.5388, -0.1742],
         [-1.1016,  0.8882, -0.2894,  ..., -0.3202, -1.4299, -1.2639],
         [-0.3970, -0.1850,  0.1407,  ..., -0.5472, -1.3482, -0.1790],
         [-1.3551,  0.0318,  0.0295,  ..., -0.0396,  1.3983, -0.7557]],

        [[ 0.3360,  1.0303,  0.0090,  ..., -0.2102,  0.4099, -0.4472],
         [-0.8487,  1.6091, -0.9103,  ...,  0.9432, -0.1886, -0.9423],
         [-3.1320, -0.0539,  0.0361,  ..., -1.0960, -2.9843, -0.3537],
         [-0.2231, -0.9749,  1.5666,  ..., -0.2318, -0.9698,  0.7510]]],
       grad_fn=<AddBackward0>)
torch.Size([2, 4, 512])
```

## 2.5 输出部分实现

- 输出部分包含:
    - 线性层
    - softmax 层

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/6.png" alt="img" style="zoom: 67%;" />

- 线性层的作用

    通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.

- softmax 层的作用

    使最后一维的向量中的数字缩放到 0-1 的概率值域内, 并满足他们的和为 1.

- 线性层和 softmax 层的代码分析:

```python
# nn.functional工具包装载了网络层中那些只进行计算, 而没有参数的层
import torch.nn.functional as F

class Generator(nn.Module):
    def __init__(self, d_model, vocab_size):
        """
        d_model: 	词嵌入维度
        vocab_size:	词表大小
        """
        super(Generator, self).__init__()
        self.project = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        # 在这里之所以使用log_softmax是因为和我们这个pytorch版本的损失函数实现有关, 在其他版本中将修复
        # log_softmax就是对softmax的结果又取了对数, 因为对数函数是单调递增函数
        # 因此对最终我们取最大的概率值没有影响
        return F.log_softmax(self.project(x), dim=-1)
```

- 调用:

```python
d_model = 512
vocab_size = 1000
x = de_result

gen = Generator(d_model, vocab_size)
gen_result = gen(x)
print(gen_result)
print(gen_result.shape)
```

- 输出效果:

```python
tensor([[[-7.2657, -7.5508, -6.0491,  ..., -6.9575, -6.2998, -7.6281],
         [-6.6701, -7.1268, -6.6308,  ..., -6.2933, -7.3609, -6.9998],
         [-7.8441, -7.3610, -7.6132,  ..., -7.0426, -7.0833, -7.7968],
         [-6.9204, -8.5056, -7.4065,  ..., -7.7902, -6.8075, -6.8057]],

        [[-8.3479, -7.2559, -6.7957,  ..., -6.8637, -6.5095, -5.9934],
         [-7.1163, -7.8364, -7.3889,  ..., -6.8037, -6.4347, -7.1798],
         [-7.1410, -6.1143, -6.7074,  ..., -8.3973, -6.5282, -7.0138],
         [-7.3398, -7.1883, -6.1667,  ..., -6.9692, -7.1996, -7.4612]]],
       grad_fn=<LogSoftmaxBackward>)
torch.Size([2, 4, 1000])
```

## 2.6 模型构建

> 通过上面的小节, 我们已经完成了所有组成部分的实现, 接下来就来实现完整的编码器 - 解码器结构.

- Transformer 总体架构图:

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/4.png" alt="img" style="zoom:50%;" />

- 编码器 - 解码器结构的代码实现:

```python
class EncoderDecoder(nn.Module):
    def __init__(self, encoder, decoder, source_embed, target_embed, generator):
        """
        encoder: 		编码器对象
        decoder: 		解码器对象 
        source_embed:	源数据嵌入函数
        target_embed:	目标数据嵌入函数
        generator:		输出部分的类别生成器对象
        """
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = source_embed
        self.tgt_embed = target_embed
        self.generator = generator

    def forward(self, source, target, source_mask, target_mask):
        """
        source: 		源数据
        target: 		目标数据
        source_mask:	源数据的掩码张量
        target_mask:	目标数据的掩码张量
        """
        return self.decode(self.encode(source, source_mask), source_mask, target, target_mask)

    def encode(self, source, source_mask):
        return self.encoder(self.src_embed(source), source_mask)

    def decode(self, memory, source_mask, target, target_mask):
        return self.decoder(self.tgt_embed(target), memory, source_mask, target_mask)
```

- 调用:

```python
vocab_size = 1000
d_model = 512
encoder = en
decoder = de
source_embed = nn.Embedding(vocab_size, d_model)
target_embed = nn.Embedding(vocab_size, d_model)
generator = gen

# 假设源数据与目标数据相同, 实际中并不相同
source = target = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))

# 假设src_mask与tgt_mask相同, 实际中并不相同
source_mask = target_mask = Variable(torch.zeros(8, 4, 4))

ed = EncoderDecoder(encoder, decoder, source_embed, target_embed, generator)
ed_result = ed(source, target, source_mask, target_mask)
print(ed_result)
print(ed_result.shape)
```

- 输出效果:

```python
tensor([[[-7.2222e-01, -1.7347e+00, -2.0005e+00,  ..., -8.5220e-01,
          -1.9407e+00, -1.3980e+00],
         [ 8.6142e-02, -1.6897e+00, -1.5596e+00,  ..., -6.4670e-01,
          -1.1912e+00, -1.7522e+00],
         [-5.1275e-01, -1.4788e+00, -1.9588e+00,  ..., -6.1676e-01,
          -8.8340e-01, -1.2669e+00],
         [-6.8181e-01, -1.9057e+00, -2.5244e+00,  ..., -1.3648e+00,
          -1.3920e+00, -4.9295e-01]],

        [[ 2.7028e-01, -1.6152e+00, -1.1730e+00,  ..., -1.0067e+00,
          -2.8657e-01,  1.1201e-03],
         [ 7.2093e-03, -6.1072e-01, -1.2369e+00,  ..., -5.3399e-02,
           5.9336e-01,  1.0195e-02],
         [ 6.5234e-01, -1.6975e+00, -1.6558e+00,  ..., -8.6709e-01,
          -3.0260e-01, -7.1283e-02],
         [ 5.7347e-01, -1.3832e+00, -2.0318e+00,  ..., -8.0464e-01,
          -6.0200e-01,  2.8472e-02]]], grad_fn=<AddBackward0>)
torch.Size([2, 4, 512])
```

---

- Tansformer 模型构建过程的代码分析

```python
def make_model(source_vocab, target_vocab, N=6, 
               d_model=512, d_ff=2048, head=8, dropout=0.1):
    """
    source_vocab:	源数据特征(词汇)总数,
    target_vocab:	目标数据特征(词汇)总数, 
    N: 				编码器和解码器堆叠数
    d_model: 		词向量映射维度
    d_ff: 			前馈全连接网络中变换矩阵的维度
    head: 			多头注意力结构中的多头数
    dropout: 		置零比率
    """
    c = copy.deepcopy

    # 实例化多头注意力类
    attn = MultiHeadedAttention(head, d_model)

    # 实例化前馈全连接类 
    ff = PositionwiseFeedForward(d_model, d_ff, dropout)

    # 实例化位置编码类
    position = PositionalEncoding(d_model, dropout)

    # 利用EncoderDecoder类实例化模型model. 分别是编码器层, 解码器层, 源数据Embedding层和位置编码组成的有序结构
    # 目标数据Embedding层和位置编码组成的有序结构, 以及类别生成器层
    # 在编码器层中有attention子层以及前馈全连接子层
    # 在解码器层中有两个attention子层以及前馈全连接层
    model = EncoderDecoder(
        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
        Decoder(DecoderLayer(d_model, c(attn), c(attn), 
                             c(ff), dropout), N),
        nn.Sequential(Embeddings(d_model, source_vocab), c(position)),
        nn.Sequential(Embeddings(d_model, target_vocab), c(position)),
        Generator(d_model, target_vocab))

    # 初始化模型中的参数, 比如线性层中的变换矩阵
    # 如果参数的维度大于1, 则将其初始化成一个服从均匀分布的矩阵
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
    return model
```

- 调用:

```python
source_vocab = 11
target_vocab = 11 
N = 6
# 其他参数都使用默认值 

if __name__ == '__main__':
    res = make_model(source_vocab, target_vocab, N)
    print(res)
```

- 输出效果:

```python
# 根据Transformer结构图构建的最终模型结构
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w1): Linear(in_features=512, out_features=2048, bias=True)
          (w2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w1): Linear(in_features=512, out_features=2048, bias=True)
          (w2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w1): Linear(in_features=512, out_features=2048, bias=True)
          (w2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w1): Linear(in_features=512, out_features=2048, bias=True)
          (w2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w1): Linear(in_features=512, out_features=2048, bias=True)
          (w2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w1): Linear(in_features=512, out_features=2048, bias=True)
          (w2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm()
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w1): Linear(in_features=512, out_features=2048, bias=True)
          (w2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w1): Linear(in_features=512, out_features=2048, bias=True)
          (w2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w1): Linear(in_features=512, out_features=2048, bias=True)
          (w2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w1): Linear(in_features=512, out_features=2048, bias=True)
          (w2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w1): Linear(in_features=512, out_features=2048, bias=True)
          (w2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w1): Linear(in_features=512, out_features=2048, bias=True)
          (w2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm()
  )
  (src_embed): Sequential(
    (0): Embeddings(
      (lut): Embedding(11, 512)
    )
    (1): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (tgt_embed): Sequential(
    (0): Embeddings(
      (lut): Embedding(11, 512)
    )
    (1): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (generator): Generator(
    (project): Linear(in_features=512, out_features=11, bias=True)
  )
)
```

## 2.7 模型基本测试运行

> 我们将通过一个小的 copy 任务完成模型的基本测试工作.

- copy 任务介绍:

    任务描述: 针对数字序列进行学习, 学习的最终目标是使输出与输入的序列相同. 如输入 `[1, 5, 8, 9, 3]`, 输出也是 `[1, 5, 8, 9, 3]`.

    任务意义: copy 任务在模型基础测试中具有重要意义, 因为 copy 操作对于模型来讲是一条明显规律, 因此模型能否在短时间内, 小数据集中学会它, 可以帮助我们断定模型所有过程是否正常, 是否已具备基本学习能力.

- 使用 copy 任务进行模型基本测试的四步曲
    1. 构建数据集生成器
    2. 获得 Transformer 模型及其优化器和损失函数
    3. 运行模型进行训练和评估
    4. 使用模型进行贪婪解码

------

1. **构建数据集生成器**

```python
# 导入工具包Batch, 它能够对原始样本数据生成对应批次的掩码张量
from pyitcast.transformer_utils import Batch  

def data_generator(V, batch_size, num_batch):
    """
    用于随机生成copy任务的数据,
    V: 			随机生成数字的最大值+1
    batch_size:	每次输送给模型更新一次参数的数据量, 经历这些样本训练后进行一次参数的更新
    num_batch:	一共输送num_batch次完成一轮
    """
    for i in range(num_batch):
        # 使用np的random.randint()方法随机生成[1, V)的整数
        # 分布在(batch_size, 10)形状的矩阵中
        data = torch.from_numpy(np.random.randint(1, V, size=(batch_size, 10)))

        # 将数据的的第一列置为1, 作为起始标志列
        # 当解码器进行第一次解码的时候, 会使用起始标志列作为输入
        data[:, 0] = 1

        # 因为是copy任务, 所有source与target是完全相同的, 且数据样本作用变量不需要求梯度
        source = Variable(data, requires_grad=False)
        target = Variable(data, requires_grad=False)

        # 使用Batch()对source和target进行对应批次的掩码张量生成, 使用yield返回
        yield Batch(source, target) 
```

- 调用:

```python
V = 11
batch = 20 
num_batch = 30

if __name__ == '__main__':
    res = data_generator(V, batch, num_batch)
    print(res)
```

- 输出效果:

```python
# 会得到一个数据生成器(生成器对象)
<generator object data_generator at 0x7fed09b49f50>
```

------

2. **获得 Transformer 模型及其优化器和损失函数**

```python
# get_std_opt用于获得标准的针对Transformer模型的优化器 
# 该标准优化器基于Adam优化器, 使其对序列到序列的任务更有效
from pyitcast.transformer_utils import get_std_opt
# 标签平滑工具包LabelSmoothing用于标签平滑, 作用是小幅度的改变原有标签值的值域
# 因为在理论上即使是人工的标注数据也并非完全正确, 会受到一些外界因素的影响而产生微小偏差
# 使用标签平滑来弥补这种偏差, 减少模型对某一条规律的绝对认知, 以防止过拟合
from pyitcast.transformer_utils import LabelSmoothing
# 损失计算工具包SimpleLossCompute能够使用标签平滑后的结果进行损失计算 (交叉熵损失函数)
from pyitcast.transformer_utils import SimpleLossCompute


# 使用make_model()获得模型的实例化对象
model = make_model(V, V, N=2)

# 使用get_std_opt()获得模型优化器
model_optimizer = get_std_opt(model)

# 使用LabelSmoothing()获得标签平滑对象
criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)

# 使用SimpleLossCompute()获得利用标签平滑结果的损失计算方法
loss = SimpleLossCompute(model.generator, criterion, model_optimizer)
```

- 标签平滑示例:

```python
from pyitcast.transformer_utils import LabelSmoothing

# 使用LabelSmoothing实例化crit对象.
# size: 目标数据的词汇总数, 即模型最后一层所得张量的最后一维大小
# padding_idx: 将tensor中的数字替换成0, padding_idx=0表示不进行替换
# smoothing: 标签的平滑程度. 如原来标签值为1, 则平滑后的值域变为[1-smoothing, 1+smoothing]
crit = LabelSmoothing(size=5, padding_idx=0, smoothing=0.5)

# 假定一个任意的模型最后输出预测结果和真实结果
predict = Variable(torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],
                                      [0, 0.2, 0.7, 0.1, 0], 
                                      [0, 0.2, 0.7, 0.1, 0]]))

target = Variable(torch.LongTensor([2, 1, 0]))
crit(predict, target)
# 绘制标签平滑图像
plt.imshow(crit.true_dist)
plt.show()
```

- 标签平滑图像:

<img src="2.Transformer%20%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/18.png" alt="img" style="zoom:50%;" />

- 标签平滑图像分析:

    我们目光集中在黄色小方块上, 它相对于横坐标横跨的值域就是标签平滑后的正向平滑值域, 我们可以看到大致是从 0.5 到 2.5.

    它相对于纵坐标横跨的值域就是标签平滑后的负向平滑值域, 我们可以看到大致是从 - 0.5 到 1.5, 总的值域空间由原来的 [0, 2] 变成了 [-0.5, 2.5].

------

3. 运行模型进行训练和评估

```python
# 单轮训练工具包run_epoch对模型使用给定的损失函数计算方法进行单轮参数更新， 并打印每轮参数更新的损失结果
from pyitcast.transformer_utils import run_epoch

def run(model, loss, epochs=10):
    """
    模型训练函数
    model: 将要进行训练的模型
    loss: 使用的损失计算方法
    epochs: 模型训练轮次数
    """
    for epoch in range(epochs):
        # 使用训练模式, 所有参数将被更新
        model.train()
        # 训练时, 传入的batch_size为20
        run_epoch(data_generator(V, 8, 20), model, loss)

        # 使用评估模式, 参数将固定不变
        model.eval()
        # 评估时, 传入的batch_size为5
        run_epoch(data_generator(V, 8, 5), model, loss)
```

- 调用:

```python
# model和loss都是来自上一步的结果
if __name__ == '__main__':
    run(model, loss)
```

- 输出效果:

```python
Epoch Step: 1 Loss: 3.410253 Tokens per Sec: 172.095871
Epoch Step: 1 Loss: 2.592789 Tokens per Sec: 218.386032
Epoch Step: 1 Loss: 2.493866 Tokens per Sec: 378.365417
Epoch Step: 1 Loss: 2.180901 Tokens per Sec: 492.175659
Epoch Step: 1 Loss: 2.391319 Tokens per Sec: 506.043213
Epoch Step: 1 Loss: 2.027400 Tokens per Sec: 421.888489
Epoch Step: 1 Loss: 2.105274 Tokens per Sec: 503.331573
Epoch Step: 1 Loss: 1.775300 Tokens per Sec: 480.145447
Epoch Step: 1 Loss: 1.860022 Tokens per Sec: 226.273895
Epoch Step: 1 Loss: 1.807518 Tokens per Sec: 449.823486
Epoch Step: 1 Loss: 1.726098 Tokens per Sec: 184.275803
Epoch Step: 1 Loss: 1.667756 Tokens per Sec: 233.092514
Epoch Step: 1 Loss: 1.821391 Tokens per Sec: 463.134186
Epoch Step: 1 Loss: 1.612317 Tokens per Sec: 410.835114
Epoch Step: 1 Loss: 1.711867 Tokens per Sec: 367.185730
Epoch Step: 1 Loss: 1.509021 Tokens per Sec: 211.591385
Epoch Step: 1 Loss: 1.574942 Tokens per Sec: 442.599884
Epoch Step: 1 Loss: 1.254890 Tokens per Sec: 431.038818
Epoch Step: 1 Loss: 1.474335 Tokens per Sec: 120.003670
Epoch Step: 1 Loss: 1.126489 Tokens per Sec: 180.745850
```

------

4. **使用模型进行贪婪解码**

```python
# 贪婪解码工具包greedy_decode对最终结进行贪婪解码, 即每次预测都选择概率最大的结果作为输出
# 这不一定能获得全局最优, 但却拥有最高的执行效率
from pyitcast.transformer_utils import greedy_decode 


def run(model, loss, epochs=10):
    for epoch in range(epochs):
        model.train()
        run_epoch(data_generator(V, 8, 20), model, loss)
        model.eval()
        run_epoch(data_generator(V, 8, 5), model, loss)

    model.eval()

    # 初始化一个输入张量
    source = Variable(torch.LongTensor([[1,3,2,5,4,6,7,8,9,10]]))

    # 初始化源数据掩码张量, 全1代表没有任何遮掩
    source_mask = Variable(torch.ones(1, 1, 10))

    # max_len: 解码的最大长度限制, 默认为10
    # start_symbol: 起始标志数字, 默认为1
    result = greedy_decode(model, source, source_mask, max_len=10, start_symbol=1)
    print(result)

if __name__ == '__main__':
    run(model, loss) 
```

- 输出效果:

```python
tensor([[ 1,  3,  2,  5, 10,  6,  9,  8,  4,  7]])
```