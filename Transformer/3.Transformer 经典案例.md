# 3. Transformer 经典案例

## 3.1 使用 Transformer 构建语言模型

- 什么是语言模型:
  
    以一个符合语言规律的序列为输入, 模型将利用序列间关系等特征, 输出一个在所有词汇上的概率分布. 这样的模型称为语言模型.

```python
# 语言模型的训练语料一般来自于文章, 对应的源文本和目标文本形如:
src1 = "I can do" tgt1 = "can do it"
src2 = "can do it", tgt2 = "do it <eos>"
```

- 语言模型能解决哪些问题:

    1. 根据语言模型的定义, 可以在它的基础上完成机器翻译, 文本生成等任务, 因为我们通过最后输出的概率分布来预测下一个词汇是什么.

    2. 语言模型可以判断输入的序列是否为一句完整的话, 因为我们可以根据输出的概率分布查看最大概率是否落在句子结束符上, 来判断完整性.

    3. 语言模型本身的训练目标是预测下一个词, 因为它的特征提取部分会抽象很多语言序列之间的关系, 这些关系可能同样对其他语言类任务有效果, 因此可以作为预训练模型进行迁移学习.

---

- 整个案例的实现可分为以下五个步骤
    1. 导入必备的工具包
    2. 导入 wikiText-2 数据集并作基本处理
    3. 构建用于模型输入的批次化数据
    4. 构建训练和评估函数
    5. 进行训练和评估 (包括验证以及测试)

1. **导入必备的工具包**

```python
import math

import torch
import torch.nn as nn
import torch.nn.functional as F

# torch中经典文本数据集有关的工具包
import torchtext
# 英文分词工具get_tokenizer
from torchtext.data.utils import get_tokenizer
# 已经构建完成的TransformerModel
from pyitcast.transformer import TransformerModel
```

- torchtext 介绍:
  
    它是 torch 工具中处理 NLP 问题的常用数据处理包.

- torchtext 的重要功能:
  
    - 对文本数据进行处理, 比如文本语料加载, 文本迭代器构建等.
    
    - 包含很多经典文本语料的预加载方法。其中包括的语料有：用于情感分析的 SST 和 IMDB, 用于问题分类的 TREC, 用于及其翻译的 WMT14,  IWSLT, 以及用于语言模型任务 wikiText-2, WikiText103, PennTreebank.

- 我们这里使用 wikiText-2 来训练语言模型, 下面有关该数据集的相关详情:

<img src="3.Transformer%20%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B.assets/19.png" alt="img" style="zoom:50%;" />

wikiText-2 数据集的体量中等, 训练集共有 600 篇短文, 共 208 万左右的词汇, 33278 个不重复词汇, OoV（有多少正常英文词汇不在该数据集中的占比）为 2.6%, 数据集中的短文都是维基百科中对一些概念的介绍和描述.

2. **导入 wikiText-2 数据集并作基本处理**

```python
# 创建语料域, 存放语料的数据结构
# tokenize: 使用get_tokenizer("basic_english")获得一个按照文本为基础英文进行分割的分割器对象, 代表给存放语料（或称作文本）施加的作用
# init_token: 对文本施加的起始符
# eos_token: 对文本施加的终止符
# lower: 存放的文本字母全部小写
# 最终获得一个Field对象
TEXT = torchtext.data.Field(tokenize=get_tokenizer("basic_english"),
                            init_token='<sos>',
                            eos_token='<eos>',
                            lower=True)
print(TEXT)

# 使用torchtext的数据集方法导入WikiText2数据
# 切分为对应训练文本, 验证文本, 测试文本, 并对这些文本施加刚刚创建的语料域
train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)
print(test_txt.examples[0].text[:10])
# 将训练集文本数据构建一个vocab对象. 后续可以调用vocab对象的stoi()方法统计文本共包含的不重复词汇总数
TEXT.build_vocab(train_txt)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

```python
<torchtext.data.field.Field object at 0x7fe01f6603d0>
['<eos>', '=', 'robert', '<unk>', '=', '<eos>', '<eos>', 'robert', '<unk>', 'is']
```

---

3. **构建用于模型输入的批次化数据**

- torch.narrow 演示:

```python
x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
print(x.narrow(0, 0, 2))
print(x.narrow(1, 1, 2))
```

```python
tensor([[1, 2, 3],
        [4, 5, 6]])
tensor([[2, 3],
        [5, 6],
        [8, 9]])
```

- 批次化过程的第一个函数 batchify 代码分析:

```python
def batchify(data, bsz):
    """
    构建批次数据的函数, 用于将文本数据映射成连续数字, 并转换成指定的样式
    data: 之前得到的文本数据 (train_txt, val_txt, test_txt)
    bsz: batch_size
    """
    # 使用TEXT的numericalize()方法将单词映射成对应的连续数字
    data = TEXT.numericalize([data.examples[0].text])
    print(data)

	# 遍历完所有数据所需要的batch数量
    nbatch = data.size(0) // bsz
    # 使用narrow()方法删除不规整的剩余数据
    # 第一个参数: 横轴删除(0)还是纵轴删除(1)
    # 第二个和第三个参数: 切割的起始位置和终止位置, 类似于切片
    data = data.narrow(0, 0, nbatch * bsz)
    print(data)

    # 使用view()方法对data进行矩阵变换 (形状为[None, bsz]), 紧接着进行转置操作
    # 如果输入是训练数据, 形状为[104335, 20], 即data的列数等于bsz
    data = data.view(bsz, -1).t().contiguous()
    return data.to(device)
batchify(test_txt, 20)
```

```python
tensor([[  3],
        [ 12],
        [635],
        ...,
        [  6],
        [  3],
        [  3]])
tensor([[   3],
        [  12],
        [ 635],
        ...,
        [  20],
        [3126],
        [7216]])
```

- batchify 的样式转化图:

<img src="3.Transformer%20%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B.assets/20.png" alt="img" style="zoom:50%;" />

大写字母 A, B, C … 代表句子中的每个单词.

- 接下来我们将使用 batchify 来处理训练数据, 验证数据以及测试数据

```python
batch_size = 20
eval_batch_size = 10

train_data = batchify(train_txt, batch_size)
val_data = batchify(val_txt, eval_batch_size)
test_data = batchify(test_txt, eval_batch_size)
```

- 上面的分割批次并没有进行源数据与目标数据的处理, 接下来我们将根据语言模型训练的语料规定来构建源数据与目标数据.

- 语言模型训练的语料规定:
  
    如果源数据为句子 ABCD, ABCD 代表句子中的词汇或符号, 则它的目标数据为 BCDE, BCDE 分别代表 ABCD 的下一个词汇.

<img src="3.Transformer%20%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B.assets/21.png" alt="img" style="zoom: 33%;" />

> 如图所示, 我们这里的句子序列是竖着的, 而且我们发现如果用一个批次处理完所有数据, 以训练数据为例, 每个句子长度高达 104335, 这明显是不科学的, 因此我们在这里要限定每个批次中的句子长度允许的最大值 bptt.

- 批次化过程的第二个函数 get_batch 代码分析:

```python
# 设定句子的最大长度
bptt = 35

def get_batch(source, i):
    """
    获得每个批次合理大小的源数据和目标数据
    source: train_data/val_data/test_data
    i: 具体的批次数
    """
    # 确定句子长度, 取bptt和len(source)-1-i中最小值
    # 实质上, 前面的批次中都会是bptt的值, 只不过最后一个批次中, 句子长度
    # 可能不够bptt的35个, 因此会变为len(source)-1-i
    seq_len = min(bptt, len(source)-1-i)

    data = source[i:i+seq_len]

    # 根据语言模型训练的语料规定, 目标数据是源数据向后移动一位
    # 因为最后目标数据的切片会越界, 使用view(-1)来保证形状正常
    target = source[i+1:i+1+seq_len].view(-1)
    return data, target
```

- 输入实例:

```python
source = test_data
i = 1
x, y = get_batch(source, i)
print(x)
print(y)
```

- 输出效果:

```python
tensor([[   12,  1053,   355,   134,    37,     7,     4,     0,   835,  9834],
        [  635,     8,     5,     5,   421,     4,    88,     8,   573,  2511],
        [    0,    58,     8,     8,     6,   692,   544,     0,   212,     5],
        [   12,     0,   105,    26,     3,     5,     6,     0,     4,    56],
        [    3, 16074, 21254,   320,     3,   262,    16,     6,  1087,    89],
        [    3,   751,  3866,    10,    12,    31,   246,   238,    79,    49],
        [  635,   943,    78,    36,    12,   475,    66,    10,     4,   924],
        [    0,  2358,    52,     4,    12,     4,     5,     0, 19831,    21],
        [   26,    38,    54,    40,  1589,  3729,  1014,     5,     8,     4],
        [   33, 17597,    33,  1661,    15,     7,     5,     0,     4,   170],
        [  335,   268,   117,     0,     0,     4,  3144,  1557,     0,   160],
        [  106,     4,  4706,  2245,    12,  1074,    13,  2105,     5,    29],
        [    5, 16074,    10,  1087,    12,   137,   251, 13238,     8,     4],
        [  394,   746,     4,     9,    12,  6032,     4,  2190,   303, 12651],
        [    8,   616,  2107,     4,     3,     4,   425,     0,    10,   510],
        [ 1339,   112,    23,   335,     3, 22251,  1162,     9,    11,     9],
        [ 1212,   468,     6,   820,     9,     7,  1231,  4202,  2866,   382],
        [    6,    24,   104,     6,     4,     4,     7,    10,     9,   588],
        [   31,   190,     0,     0,   230,   267,     4,   273,   278,     6],
        [   34,    25,    47,    26,  1864,     6,   694,     0,  2112,     3],
        [   11,     6,    52,   798,     8,    69,    20,    31,    63,     9],
        [ 1800,    25,  2141,  2442,   117,    31,   196,  7290,     4,   298],
        [   15,   171,    15,    17,  1712,    13,   217,    59,   736,     5],
        [ 4210,   191,   142,    14,  5251,   939,    59,    38, 10055, 25132],
        [  302,    23, 11718,    11,    11,   599,   382,   317,     8,    13],
        [   16,  1564,     9,  4808,     6,     0,     6,     6,     4,     4],
        [    4,     7,    39,     7,  3934,     5,     9,     3,  8047,   557],
        [  394,     0, 10715,  3580,  8682,    31,   242,     0, 10055,   170],
        [   96,     6,   144,  3403,     4,    13,  1014,    14,     6,  2395],
        [    4,     3, 13729,    14,    40,     0,     5,    18,   676,  3267],
        [ 1031,     3,     0,   628,  1589,    22, 10916, 10969,     5, 22548],
        [    9,    12,     6,    84,    15,    49,  3144,     7,   102,    15],
        [  916,    12,     4,   203,     0,   273,   303,   333,  4318,     0],
        [    6,    12,     0,  4842,     5,    17,     4,    47,  4138,  2072],
        [   38,   237,     5,    50,    35,    27, 18530,   244,    20,     6]],
       device='cuda:0')
tensor([  635,     8,     5,     5,   421,     4,    88,     8,   573,  2511,
            0,    58,     8,     8,     6,   692,   544,     0,   212,     5,
           12,     0,   105,    26,     3,     5,     6,     0,     4,    56,
            3, 16074, 21254,   320,     3,   262,    16,     6,  1087,    89,
            3,   751,  3866,    10,    12,    31,   246,   238,    79,    49,
          635,   943,    78,    36,    12,   475,    66,    10,     4,   924,
            0,  2358,    52,     4,    12,     4,     5,     0, 19831,    21,
           26,    38,    54,    40,  1589,  3729,  1014,     5,     8,     4,
           33, 17597,    33,  1661,    15,     7,     5,     0,     4,   170,
          335,   268,   117,     0,     0,     4,  3144,  1557,     0,   160,
          106,     4,  4706,  2245,    12,  1074,    13,  2105,     5,    29,
            5, 16074,    10,  1087,    12,   137,   251, 13238,     8,     4,
          394,   746,     4,     9,    12,  6032,     4,  2190,   303, 12651,
            8,   616,  2107,     4,     3,     4,   425,     0,    10,   510,
         1339,   112,    23,   335,     3, 22251,  1162,     9,    11,     9,
         1212,   468,     6,   820,     9,     7,  1231,  4202,  2866,   382,
            6,    24,   104,     6,     4,     4,     7,    10,     9,   588,
           31,   190,     0,     0,   230,   267,     4,   273,   278,     6,
           34,    25,    47,    26,  1864,     6,   694,     0,  2112,     3,
           11,     6,    52,   798,     8,    69,    20,    31,    63,     9,
         1800,    25,  2141,  2442,   117,    31,   196,  7290,     4,   298,
           15,   171,    15,    17,  1712,    13,   217,    59,   736,     5,
         4210,   191,   142,    14,  5251,   939,    59,    38, 10055, 25132,
          302,    23, 11718,    11,    11,   599,   382,   317,     8,    13,
           16,  1564,     9,  4808,     6,     0,     6,     6,     4,     4,
            4,     7,    39,     7,  3934,     5,     9,     3,  8047,   557,
          394,     0, 10715,  3580,  8682,    31,   242,     0, 10055,   170,
           96,     6,   144,  3403,     4,    13,  1014,    14,     6,  2395,
            4,     3, 13729,    14,    40,     0,     5,    18,   676,  3267,
         1031,     3,     0,   628,  1589,    22, 10916, 10969,     5, 22548,
            9,    12,     6,    84,    15,    49,  3144,     7,   102,    15,
          916,    12,     4,   203,     0,   273,   303,   333,  4318,     0,
            6,    12,     0,  4842,     5,    17,     4,    47,  4138,  2072,
           38,   237,     5,    50,    35,    27, 18530,   244,    20,     6,
           13,  1083,    35,  1990,   653,    13,    10,    11,  1538,    56],
       device='cuda:0')
```

---

4. **构建训练和评估函数**

- 设置模型超参数和初始化模型

```python
# 通过TEXT.vocab.stoi()方法获得不重复词汇总数
ntokens = len(TEXT.vocab.stoi)
# 词嵌入大小为200
emsize = 200
# 前馈全连接层的节点数
nhid = 200
# 编码器层的数量
nlayers = 2
# 多头注意力机制的头数
nhead = 2
dropout = 0.2

# 将参数输入到TransformerModel中实例化模型
model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)

# 交叉熵损失函数
criterion = nn.CrossEntropyLoss()
lr = 5.0
# SGD优化器
optimizer = torch.optim.SGD(model.parameters(), lr=lr)

# 学习率调整器
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)
```

- 模型训练代码分析:

```python
import time

def train():
    model.train()
    total_loss = 0
	log_interval = 200
    start_time = time.time()

    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):
        # 通过get_batch()方法获得源数据和目标数据
        data, targets = get_batch(train_data, i)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output.view(-1, ntokens), targets)
        loss.backward()
        # 使用nn自带的clip_grad_norm_()方法进行梯度规范化, 防止出现梯度消失或爆炸
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        optimizer.step()
        total_loss += loss.item()

        if batch % log_interval == 0 and batch > 0:
            cur_loss = total_loss / log_interval
            elapsed = time.time() - start_time
            # 打印轮数, 当前批次, 总批次, 当前学习率, 训练速度(每豪秒处理多少批次),
      		# 平均损失, 以及困惑度. 困惑度是衡量语言模型的重要指标, 其计算方法是
      		# 对交叉熵平均损失取自然对数的幂
            print('| epoch {:3d} | {:5d}/{:5d} batches | '
                  'lr {:02.2f} | ms/batch {:5.2f} | '
                  'loss {:5.2f} | ppl {:8.2f}'.format(
                    epoch, batch, len(train_data) // bptt, scheduler.get_last_lr()[0],
                    elapsed * 1000 / log_interval,
                    cur_loss, math.exp(cur_loss)))

            total_loss = 0
            start_time = time.time()
```

- 模型评估代码分析:

```python
def evaluate(eval_model, data_source):
    """
    eval_model: 每轮训练产生的模型
    data_source: 验证或测试数据集
    """
    eval_model.eval()
    total_loss = 0

    with torch.no_grad():
        # 与训练过程相同, 但是因为过程不需要打印信息, 因此不需要batch数
        for i in range(0, data_source.size(0) - 1, bptt):
            # 通过get_batch()方法获得源数据和目标数据
            data, targets = get_batch(data_source, i)
            output = eval_model(data)
            # 对输出形状扁平化, 变为全部词汇的概率分布
            output_flat = output.view(-1, ntokens)
            total_loss += criterion(output_flat, targets).item()
            cur_loss = total_loss / ((data_source.size(0) - 1) / bptt)
    return cur_loss
```

---

5. **进行训练和评估 (包括验证以及测试)**

- 模型的训练与验证代码分析:

```python
best_val_loss = float("inf")
epochs = 3
best_model = None
best_model_path = './models/transformer_lang.pth'

if not os.path.exists(best_model_path):
    for epoch in range(1, epochs + 1):
        epoch_start_time = time.time()
        train()
        val_loss = evaluate(model, val_data)
        print('-' * 89)
        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '
              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),
                                         val_loss, math.exp(val_loss)))
        print('-' * 89)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            # 使用深拷贝, 拷贝最优模型
            best_model = copy.deepcopy(model)
            torch.save(model.state_dict(), best_model_path)
            # 每轮都会对优化方法的学习率做调整
            scheduler.step()
else:
    model.load_state_dict(torch.load(best_model_path))
```

- 输出效果:

```python
| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 28.21 | loss  8.07 | ppl  3191.51
| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 27.17 | loss  6.81 | ppl   909.66
| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 28.37 | loss  6.36 | ppl   578.45
| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 27.75 | loss  6.23 | ppl   507.09
| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 27.45 | loss  6.12 | ppl   456.96
| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 27.96 | loss  6.08 | ppl   438.96
| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 26.75 | loss  6.04 | ppl   421.28
| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 27.30 | loss  6.05 | ppl   423.43
| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 27.72 | loss  5.96 | ppl   385.75
| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 27.87 | loss  5.96 | ppl   386.78
| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 27.23 | loss  5.85 | ppl   347.59
| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 28.09 | loss  5.90 | ppl   363.27
| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 28.08 | loss  5.91 | ppl   367.48
| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 28.14 | loss  5.80 | ppl   329.21
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 85.91s | valid loss  5.73 | valid ppl   309.02
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 27.86 | loss  5.80 | ppl   329.33
| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 28.40 | loss  5.77 | ppl   322.04
| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 28.03 | loss  5.60 | ppl   269.63
| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 27.47 | loss  5.64 | ppl   280.29
| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 28.35 | loss  5.58 | ppl   266.23
| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 28.76 | loss  5.62 | ppl   274.80
| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 29.38 | loss  5.62 | ppl   276.31
| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 29.56 | loss  5.65 | ppl   284.83
| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 27.89 | loss  5.58 | ppl   264.28
| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 28.56 | loss  5.61 | ppl   273.53
| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 29.21 | loss  5.50 | ppl   244.88
| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 29.47 | loss  5.57 | ppl   261.71
| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 29.91 | loss  5.58 | ppl   264.60
| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 28.60 | loss  5.51 | ppl   246.45
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 89.50s | valid loss  5.57 | valid ppl   262.25
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 29.77 | loss  5.54 | ppl   255.70
| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 29.98 | loss  5.55 | ppl   256.60
| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 29.57 | loss  5.36 | ppl   212.30
| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 28.99 | loss  5.42 | ppl   225.83
| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 28.57 | loss  5.38 | ppl   217.18
| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 28.61 | loss  5.40 | ppl   222.44
| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 28.44 | loss  5.43 | ppl   228.67
| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 29.55 | loss  5.47 | ppl   238.11
| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 28.92 | loss  5.41 | ppl   222.54
| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 28.96 | loss  5.44 | ppl   230.50
| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 29.07 | loss  5.33 | ppl   206.23
| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 29.22 | loss  5.41 | ppl   222.59
| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 28.99 | loss  5.41 | ppl   223.37
| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 29.11 | loss  5.34 | ppl   207.89
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 90.20s | valid loss  5.59 | valid ppl   267.96
-----------------------------------------------------------------------------------------
```

- 模型测试代码分析:

```python
best_model = model
test_loss = evaluate(best_model, test_data)

print('=' * 89)
print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(
    test_loss, math.exp(test_loss)))
print('=' * 89)
```

- 输出效果:

```python
=========================================================================================
| End of training | test loss  5.47 | test ppl   237.61
=========================================================================================
```
