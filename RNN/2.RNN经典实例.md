# 2. RNN经典实例

[toc]

## 2.1 使用 RNN 模型构建人名分类器

- 关于人名分类问题:
  
    以一个人名为输入, 使用模型帮助我们判断它最有可能是来自哪一个国家的人名, 这在某些国际化公司的业务中具有重要意义, 在用户注册过程中, 会根据用户填写的名字直接给他分配可能的国家或地区选项, 以及该国家或地区的国旗, 限制手机号码位数等等.

- 人名分类数据:

    数据下载地址: https://download.pytorch.org/tutorial/data.zip

- 数据文件预览:

```
- data/
    - names/
        Arabic.txt
        Chinese.txt
        Czech.txt
        Dutch.txt
        English.txt
        French.txt
        German.txt
        Greek.txt
        Irish.txt
        Italian.txt
        Japanese.txt
        Korean.txt
        Polish.txt
        Portuguese.txt
        Russian.txt
        Scottish.txt
        Spanish.txt
        Vietnamese.txt
```

- Chiness.txt 预览:

```
Ang
Au-Yong
Bai
Ban
Bao
Bei
Bian
Bui
Cai
Cao
Cen
Chai
Chaim
Chan
Chang
Chao
Che
Chen
Cheng
```

---

- 整个案例的实现可分为以下五个步骤:

    1. 导入必备的工具包.

    2. 对 data 文件中的数据进行处理, 满足训练要求.

    3. 构建 RNN 模型 (包括传统 RNN, LSTM 以及 GRU).

    4. 构建训练函数并进行训练.

    5. 构建评估函数并进行预测.

1. **导入必备的工具包**

> python 版本使用 3.6.x, pytorch 版本使用 1.3.1

```shell
pip install torch==1.3.1
```

```python
# 从io中导入文件打开方法
from io import open
# 帮助使用正则表达式进行子目录的查询
import glob
import os
# 用于获得常见字母及字符规范化
import string
import unicodedata
# 导入随机工具random
import random
# 导入时间和数学工具包
import time
import math
# 导入torch工具
import torch
# 导入nn准备构建模型
import torch.nn as nn
# 引入制图工具包        
import matplotlib.pyplot as plt
```

2. **对 data 文件中的数据进行处理, 满足训练要求**

- 获取常用的字符数量:

```python
# 所有常用字符包括字母和常用标点
all_letters = string.ascii_letters + " .,;'"
# 常用字符数量
n_letters = len(all_letters)
print("n_letter:", n_letters)
```

- 输出效果:

```python
n_letter: 57
```

---

- 字符规范化之 unicode 转 Ascii 函数:

```python
# 去掉一些语言中的重音标记
# 如: Ślusàrski ---> Slusarski
def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn'
        and c in all_letters
    )
```

```python
s = "Ślusàrski"
a = unicodeToAscii(s)
print(a)
```

- 输出效果:

```python
Slusarski
```

---

- 构建一个从持久化文件中读取内容到内存的函数:

```python
data_path = "./data/name/"

def readLines(filename):
    """从文件中读取每一行加载到内存中形成列表"""
    # 打开指定文件并读取所有内容, 使用strip()去除两侧空白符, 以'\n'进行切分
    lines = open(filename, encoding='utf-8').read().strip().split('\n')
    # 对每一个lines列表中的名字进行Ascii转换, 使其规范化. 最后返回一个名字列表
    return [unicodeToAscii(line) for line in lines]
```

```python
filename = data_path + "Chinese.txt"
lines = readLines(filename)
print(lines[:20])
```

- 输出效果:

```python
['Ang', 'AuYong', 'Bai', 'Ban', 'Bao', 'Bei', 'Bian', 'Bui', 'Cai', 'Cao', 'Cen', 'Chai', 'Chaim', 'Chan', 'Chang', 'Chao', 'Che', 'Chen', 'Cheng', 'Cheung']
```

---

- 构建人名类别 (所属的语言) 列表与人名对应关系字典:

```python
# 构建人名类别与具体人名对应关系的字典. 形如：{"English":["Lily", "Susan", "Kobe"], "Chinese":["Zhang San", "Xiao Ming"]}
category_lines = {}

# 构建所有类别的列表. 形如： ["English",...,"Chinese"]
all_categories = []

# 读取指定路径下的txt文件, glob支持正则表达式
for filename in glob.glob(data_path + '*.txt'):
    category = os.path.splitext(os.path.basename(filename))[0]
    all_categories.append(category)
    # 读取每个文件的内容, 形成名字列表
    lines = readLines(filename)
    # 按照对应的类别, 将名字列表写入到category_lines字典中
    category_lines[category] = lines

n_categories = len(all_categories)
print("n_categories:", n_categories)

print(category_lines['Italian'][:10])
```

- 输出效果:

```python
n_categories 18
['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni', 'Abatescianni', 'Abba', 'Abbadelli', 'Abbascia', 'Abbatangelo']
```

---

- 将人名转化为对应 onehot 张量表示:

```python
def lineToTensor(line):
    """将人名转化为对应onehot张量表示, 参数line是输入的人名"""
    # 初始化一个形状为len(line)*1*n_letters的全0张量
    # 代表人名中的每个字母用一个1*n_letters的张量表示
    tensor = torch.zeros(len(line), 1, n_letters)
    for li, letter in enumerate(line):
        # 使用字符串的find()方法得到每个字符在all_letters中的索引
        # 也是所生成onehot张量中1的索引位置
        tensor[li][0][all_letters.find(letter)] = 1

    return tensor
```

```python
line = "Bai"
line_tensor = lineToTensor(line)
print("line_tensot:", line_tensor)
```

- 输出效果:

```python
line_tensor: tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0.]],

        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0.]]])
```

3. **构建 RNN 模型**

- 构建传统的 RNN 模型:

```python
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        """
        input_size:		RNN输入的最后一个维度
        hidden_size: 	RNN隐藏层的最后一个维度
        output_size: 	RNN网络最后线性层的输出维度
        num_layers:		RNN网络的层数
        """
        super(RNN, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_layers = num_layers

        # 实例化预定义的nn.RNN
        self.rnn = nn.RNN(input_size, hidden_size, num_layers)
        # 实例化全连接线性层, 将nn.RNN的输出维度转化为指定的输出维度
        self.linear = nn.Linear(hidden_size, output_size)
        # 实例化nn中预定的Softmax层, 用于从输出层获得类别结果
        self.softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x, hidden):
        """
        x:		人名分类器中的输入张量 (1*n_letters)
        hidden:	RNN的隐藏层张量 (self.num_layers*1*self.hidden_size)
        """
        # nn.RNN要求输入是三维张量
        x = x.unsqueeze(0)
        # 如果num_layers=1, rr恒等于hn
        rr, hn = self.rnn(x, hidden)
        # 返回hn作为后续RNN的输入
        return self.softmax(self.linear(rr)), hn

    def initHidden(self):
        """初始化隐层张量"""
        # self.num_layers*1*self.hidden_size     
        return torch.zeros(self.num_layers, 1, self.hidden_size)  
```

- `torch.unsqueeze` 演示:

```python
x = torch.tensor([1, 2, 3, 4])
print(x.shape)
y = torch.unsqueeze(x, 0)
print(y.shape)
z = torch.unsqueeze(x, 1)
print(z)
print(z.shape)
```

```python
torch.Size([4])
torch.Size([1, 4])
tensor([[1],
        [2],
        [3],
        [4]])
torch.Size([4, 1])
```

---

- 构建 LSTM 模型:

```python
class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        """
        input_size:		LSTM输入的最后一个维度
        hidden_size: 	LSTM隐藏层的最后一个维度
        output_size: 	LSTM网络最后线性层的输出维度
        num_layers: 	LSTM网络的层数
        """
        super(LSTM, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_layers = num_layers

        # 实例化预定义的nn.LSTM
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)
        # 实例化全连接线性层, 作用是将nn.LSTM的输出维度转化为指定的输出维度
        self.linear = nn.Linear(hidden_size, output_size)
        # 实例化nn中预定的Softmax层, 用于从输出层获得类别结果
        self.softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x, hidden, c):
        """注意: LSTM网络的输入有3个张量，尤其不要忘记细胞状态c"""
        x = x.unsqueeze(0)
        rr, (hn, c) = self.lstm(x, (hidden, c))
        return self.softmax(self.linear(rr)), hn, c

    def initHiddenAndC(self):  
        """初始化函数不仅初始化hidden还要初始化细胞状态c, 它们形状相同"""
        c = hidden = torch.zeros(self.num_layers, 1, self.hidden_size)
        return hidden, c
```

- 构建 GRU 模型:

```python
class GRU(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        """
        input_size:		GRU输入的最后一个维度
        hidden_size: 	GRU隐藏层的最后一个维度
        output_size: 	GRU网络最后线性层的输出维度
        num_layers: 	GRU网络的层数
        """
        super(GRU, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_layers = num_layers

        self.gru = nn.GRU(input_size, hidden_size, num_layers)
        self.linear = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x, hidden):
        x = x.unsqueeze(0)
        rr, hn = self.gru(x, hidden)
        return self.softmax(self.linear(rr)), hn

    def initHidden(self):
        return torch.zeros(self.num_layers, 1, self.hidden_size)
```

- 调用:

```python
# 因为是onehot编码, 输入张量最后一维的尺寸为n_letters
input_size = n_letters
# 定义隐层的最后一维尺寸
n_hidden = 128
# 输出尺寸为语言类别总数n_categories
output_size = n_categories
# num_layer使用默认值, num_layers = 1

# 假如以一个字母B作为RNN的首次输入, 它通过lineToTensor转为张量
# 而lineToTensor输出是三维张量, RNN类需要二维张量
# 因此需要使用squeeze(0)降低一个维度
x = lineToTensor('B').squeeze(0)
# 初始化一个三维的隐层0张量, 也是初始的细胞状态张量
hidden = c = torch.zeros(1, 1, n_hidden)

rnn = RNN(input_size, n_hidden, output_size)
lstm = LSTM(input_size, n_hidden, output_size)
gru = GRU(input_size, n_hidden, output_size)

rnn_output, next_hidden = rnn(x, hidden)
print("rnn:", rnn_output)
print("rnn_shape:", rnn_output.shape)
print('**********')

lstm_output, next_hidden, c = lstm(x, hidden, c)
print("lstm:", lstm_output)
print("lstm_shape:", lstm_output.shape)
print('**********')

gru_output, next_hidden = gru(x, hidden)
print("gru:", gru_output)
print("gru_shape:", gru_output.shape)
```

- 输出效果:

```python
rnn: tensor([[[-2.8129, -2.8504, -2.7314, -2.9458, -2.8221, -2.8283, -2.9747,
          -2.9682, -2.9622, -2.8577, -2.9929, -3.0224, -3.0299, -2.7786,
          -2.9321, -2.8436, -2.8241, -2.9156]]], grad_fn=<LogSoftmaxBackward>)
rnn_shape: torch.Size([1, 1, 18])
**********
lstm: tensor([[[-2.8423, -2.9190, -2.9386, -2.9748, -2.9585, -2.7992, -2.8656,
          -2.8893, -2.9086, -2.9250, -2.9528, -2.8401, -2.8417, -2.8762,
          -2.8663, -2.8391, -2.8658, -2.9457]]], grad_fn=<LogSoftmaxBackward>)
lstm_shape: torch.Size([1, 1, 18])
**********
gru: tensor([[[-2.9277, -2.9667, -2.8257, -2.8768, -2.8913, -2.8772, -2.7976,
          -2.9693, -2.9540, -2.9299, -2.9477, -2.8493, -2.8904, -2.9280,
          -2.8275, -2.9278, -2.8095, -2.8563]]], grad_fn=<LogSoftmaxBackward>)
gru_shape: torch.Size([1, 1, 18])
**********
```

4. **构建训练函数并进行训练**

- 从输出结果中获得指定类别函数:

```python
def categoryFromOutput(output):
    """从输出结果中获得指定类别, 参数为输出张量output"""
    # 从输出张量中返回最大的值和索引对象
    top_n, top_i = output.topk(1)
    # top_i对象中取出索引的值
    category_i = top_i[0].item()
    # 根据索引值获得对应语言类别, 返回语言类别和索引值
    return all_categories[category_i], category_i
```

- `torch.topk` 演示:

```python
x = torch.arange(1., 6.)
print(x)
res = torch.topk(x, 3)
print(res)
```

```python
tensor([1., 2., 3., 4., 5.])
torch.return_types.topk(
values=tensor([5., 4., 3.]),
indices=tensor([4, 3, 2]))
```

- 输入参数:

```python
category, category_i = categoryFromOutput(gru_output)
print("category:", category) 
print("category_i:", category_i)
```

- 输出效果:

```python
category: Arabic
category_i: 0
```

---

- 随机生成训练数据:

```python
def randomTrainingExample():
    """该函数用于随机产生训练数据"""
    # 第一步使用random.choice()方法从all_categories中随机选择一个类别
    category = random.choice(all_categories)
    # 第二步通过category_lines字典取category类别对应的名字列表，从列表中随机取一个名字
    line = random.choice(category_lines[category])
    # 第三部将该类别在所有类别列表中的索引封装成tensor
    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)
    # 第四步将随机取到的名字通过函数lineToTensor()转化为onehot张量表示
    line_tensor = lineToTensor(line)
    return category, line, category_tensor, line_tensor
```

```python
for i in range(10):
    category, line, category_tensor, line_tensor = randomTrainingExample()
    print('category =', category, '/ line =', line, '/ category_tensor =', category_tensor)
print(line_tensor)
```

- 输出效果:

```python
category = Scottish / line = Shaw / category_tensor = tensor([15])
category = Arabic / line = Bata / category_tensor = tensor([0])
category = Portuguese / line = Romao / category_tensor = tensor([13])
category = Japanese / line = Iriye / category_tensor = tensor([10])
category = English / line = Fagan / category_tensor = tensor([4])
category = Japanese / line = Hike / category_tensor = tensor([10])
category = Portuguese / line = Simoes / category_tensor = tensor([13])
category = French / line = Villeneuve / category_tensor = tensor([5])
category = Greek / line = Theofilopoulos / category_tensor = tensor([7])
category = Portuguese / line = Coelho / category_tensor = tensor([13])
tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0.]]])
```

---

- 构建传统 RNN 训练函数:

```python
# RNN的最后一层为nn.LogSoftmax(), 因此选定损失函数为nn.NLLLoss(), 两者的内部计算逻辑正好吻合
criterion = nn.NLLLoss()
learning_rate = 0.005 

def trainRNN(category_tensor, line_tensor):
    """
    category_tensor:	类别的张量表示, 训练数据的标签,
    line_tensor:		名字的张量表示, 训练数据的标签
    """
    # 通过实例化对象rnn初始化隐层张量
    hidden = rnn.initHidden()

    # 关键的一步: 将模型结构中的梯度归0
    rnn.zero_grad()

    # 循环遍历训练数据line_tensor中的每个字符，逐个传入rnn之中, 并且迭代更新hidden
    for i in range(line_tensor.size()[0]):
        output, hidden = rnn(line_tensor[i], hidden)

    # 因为rnn对象由nn.RNN实例化得到, 最终输出形状是三维张量, 为了满足于category_tensor
    # 进行对比计算损失, 需要进行降维操作
    loss = criterion(output.squeeze(0), category_tensor)
    loss.backward()

    # 更新模型中所有的参数
    for p in rnn.parameters():
        # 将参数的张量表示与参数的梯度乘以学习率的结果相加以此来更新参数
        p.data.add_(p.grad.data, alpha=-learning_rate)
    # 返回结果和损失的值
    return output, loss.item()
```

- `torch.add` 演示:

```python
a = torch.randn(4)
print(a)
b = torch.randn(4, 1)
print(b)
c = torch.add(a, b, alpha=10)
print(c)
```

```python
tensor([ 0.4102, -1.1195,  0.1415, -1.6078])
tensor([[0.2756],
        [1.0030],
        [1.6276],
        [0.2164]])
tensor([[ 3.1658,  1.6362,  2.8971,  1.1478],
        [10.4400,  8.9103, 10.1712,  8.4219],
        [16.6859, 15.1563, 16.4172, 14.6679],
        [ 2.5745,  1.0449,  2.3058,  0.5565]])
```

- 构建 LSTM 训练函数:

```python
# 与传统RNN相比多出细胞状态c
def trainLSTM(category_tensor, line_tensor):
    hidden, c = lstm.initHiddenAndC()
    lstm.zero_grad()
    for i in range(line_tensor.size()[0]):
        # 返回output, hidden以及细胞状态c
        output, hidden, c = lstm(line_tensor[i], hidden, c)
    loss = criterion(output.squeeze(0), category_tensor)
    loss.backward()

    for p in lstm.parameters():
        p.data.add_(p.grad.data, alpha=-learning_rate)
    return output, loss.item()
```

- 构建 GRU 训练函数:

```python
def trainGRU(category_tensor, line_tensor):
    hidden = gru.initHidden()
    gru.zero_grad()
    for i in range(line_tensor.size()[0]):
        output, hidden= gru(line_tensor[i], hidden)
    loss = criterion(output.squeeze(0), category_tensor)
    loss.backward()

    for p in gru.parameters():
        p.data.add_(p.grad.data, alpha=-learning_rate)
    return output, loss.item()
```

---

- 构建时间计算函数:

```python
def timeSince(since):
    "获得每次打印的训练耗时, since是训练开始时间"
    # 获得当前时间
    now = time.time()
    # 获得时间差, 就是训练耗时
    s = now - since
    # 将秒转化为分钟, 并取整
    m = math.floor(s / 60)
    # 计算剩下不够凑成1分钟的秒数
    s -= m * 60
    # 返回指定格式的耗时
    return '%dm %ds' % (m, s)
```

- 输入参数:

```python
# 假定模型训练开始时间是10min之前
since = time.time() - 10*60
period = timeSince(since)
print(period)
```

- 输出效果:

```python
10m 0s
```

---

- 构建训练过程的日志打印函数:

```python
# 设置训练迭代次数
n_iters = 100000
# 设置结果的打印间隔
print_every = 5000
# 设置绘制损失曲线上的制图间隔
plot_every = 1000

def train(train_type_fn):
    """
    训练过程的日志打印函数
    train_type_fn: 代表选择哪种模型训练函数, 如trainRNN
    """
    # 初始化存储每个制图间隔损失的列表
    all_losses = []
    # 获得训练开始时间戳
    start = time.time()
    # 设置初始间隔损失为0
    current_loss = 0
    # 从1开始进行训练迭代, 共n_iters次 
    for iter in range(1, n_iters+1):
        # 通过randomTrainingExample函数随机获取一组训练数据和对应的类别
        category, line, category_tensor, line_tensor = randomTrainingExample()
        # 将训练数据和对应类别的张量表示传入到train函数中
        output, loss = train_type_fn(category_tensor, line_tensor)      
        # 计算制图间隔中的总损失
        current_loss += loss   

        # 如果迭代数能够整除打印间隔
        if iter % print_every == 0:
            # 取该迭代步上的output通过categoryFromOutput函数获得对应的类别和类别索引
            guess, guess_i = categoryFromOutput(output)
            # 和真实的类别category做比较, 如果相同则打对号, 否则打叉号.
            correct = '✓' if guess == category else '✗ (%s)' % category
            # 打印迭代步, 迭代步百分比, 当前训练耗时, 损失, 该步预测的名字, 以及是否正确                                
            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter/n_iters*100, timeSince(start), loss, line, guess, correct))

        # 如果迭代数能够整除制图间隔
        if iter % plot_every == 0:
            # 将保存该间隔中的平均损失到all_losses列表中
            all_losses.append(current_loss / plot_every)
            # 间隔损失重置为0
            current_loss = 0
    # 返回对应的总损失列表和训练耗时
    return all_losses, int(time.time() - start)
```

- 开始训练传统 RNN, LSTM, GRU 模型并制作对比图:

```python
# 调用train函数, 分别进行RNN, LSTM, GRU模型的训练
# 并返回各自的全部损失, 以及训练耗时用于制图
RNN_PATH = './models/RNN_classifier.pth'
LSTM_PATH = './models/LSTM_classifier.pth'
GRU_PATH = './models/GRU_classifier.pth'
if not (os.path.exists(RNN_PATH) and os.path.exists(LSTM_PATH) and os.path.exists(GRU_PATH)):
    all_losses1, period1 = train(trainRNN)
    all_losses2, period2 = train(trainLSTM)
    all_losses3, period3 = train(trainGRU)

    torch.save(rnn.state_dict(), RNN_PATH)
    torch.save(lstm.state_dict(), LSTM_PATH)
    torch.save(gru.state_dict(), GRU_PATH)

    # 绘制损失对比曲线, 训练耗时对比柱张图
    # 创建画布0
    plt.figure(0)
    # 绘制损失对比曲线
    plt.plot(all_losses1, label="RNN")
    plt.plot(all_losses2, color="red", label="LSTM")
    plt.plot(all_losses3, color="orange", label="GRU")
    plt.legend(loc='upper left')
    plt.savefig("./figures/loss_compare.png")

    # 创建画布1
    plt.figure(1)
    x_data=["RNN", "LSTM", "GRU"] 
    y_data = [period1, period2, period3]
    # 绘制训练耗时对比柱状图
    plt.bar(range(len(x_data)), y_data, tick_label=x_data)
    plt.savefig("./figures/period_compare.png")
else:
    rnn.load_state_dict(torch.load(RNN_PATH))
    lstm.load_state_dict(torch.load(LSTM_PATH))
    gru.load_state_dict(torch.load(GRU_PATH))
```

- 传统 RNN 训练日志输出:

```python
5000 5% (0m 24s) 1.9952 Jaeger / German ✓
10000 10% (0m 49s) 1.8826 Knopp / German ✓
15000 15% (1m 15s) 0.3097 Sekigawa / Japanese ✓
20000 20% (1m 39s) 3.2835 Eustis / Arabic ✗ (French)
25000 25% (2m 3s) 1.0103 Ventura / Spanish ✗ (Portuguese)
30000 30% (2m 30s) 1.7755 Rozinek / Polish ✗ (Czech)
35000 35% (2m 53s) 2.5037 Seif / Scottish ✗ (Arabic)
40000 40% (3m 20s) 0.8114 Derzhavin / Russian ✓
45000 45% (3m 47s) 0.1860 Macshuibhne / Irish ✓
50000 50% (4m 15s) 1.5372 Cassidy / English ✗ (Irish)
55000 55% (4m 47s) 2.5549 Masters / Dutch ✗ (English)
60000 60% (5m 17s) 1.0483 Lobo / Portuguese ✓
65000 65% (5m 43s) 0.9233 Mansfield / English ✓
70000 70% (6m 8s) 0.1975 Rutkowski / Polish ✓
75000 75% (6m 31s) 0.0337 Janowski / Polish ✓
80000 80% (6m 52s) 2.8250 Wirt / Scottish ✗ (German)
85000 85% (7m 13s) 0.6126 Svoboda / Czech ✓
90000 90% (7m 37s) 3.2038 Frei / Portuguese ✗ (German)
95000 95% (8m 4s) 0.1836 Ishihara / Japanese ✓
100000 100% (8m 31s) 0.6537 Kohler / German ✓
```

- LSTM 训练日志输出:

```python
5000 5% (0m 44s) 2.9168 Villanueva / Dutch ✗ (Spanish)
10000 10% (1m 29s) 2.8107 Garcia / Japanese ✗ (Portuguese)
15000 15% (2m 8s) 2.9135 Hawlata / Greek ✗ (Czech)
20000 20% (2m 46s) 2.7594 Nieves / Greek ✗ (Spanish)
25000 25% (3m 26s) 2.5000 Yasui / Arabic ✗ (Japanese)
30000 30% (4m 15s) 2.6626 Stepan / Irish ✗ (Czech)
35000 35% (5m 3s) 2.7359 Grozmanova / Polish ✗ (Czech)
40000 40% (5m 50s) 1.9262 Haddad / Arabic ✓
45000 45% (6m 33s) 2.5644 Siemon / Irish ✗ (German)
50000 50% (7m 15s) 1.9457 Azarola / Polish ✗ (Spanish)
55000 55% (7m 57s) 0.8028 Fermi / Italian ✓
60000 60% (8m 40s) 0.2397 Dioletis / Greek ✓
65000 65% (9m 17s) 1.4950 Strobel / German ✓
70000 70% (9m 53s) 1.2936 Banos / Greek ✓
75000 75% (10m 32s) 0.0746 Dzhabrailov / Russian ✓
80000 80% (11m 11s) 2.5985 Black / Czech ✗ (Scottish)
85000 85% (11m 51s) 0.6768 Coilean / Irish ✓
90000 90% (12m 31s) 1.1962 an / Chinese ✗ (Vietnamese)
95000 95% (13m 9s) 2.4661 Attia / Spanish ✗ (Arabic)
100000 100% (13m 42s) 0.1854 Glynatsis / Greek ✓
```

- GRU 训练日志输出:

```python
5000 5% (0m 30s) 2.8697 Jeleznyak / Polish ✗ (Russian)
10000 10% (1m 3s) 2.7386 Finney / Polish ✗ (English)
15000 15% (1m 38s) 1.1385 Papadopulos / Greek ✓
20000 20% (2m 15s) 2.7130 Lutohin / Irish ✗ (Russian)
25000 25% (2m 53s) 2.7325 Salomon / Russian ✗ (French)
30000 30% (3m 32s) 1.3994 Ojeda / Spanish ✓
35000 35% (4m 6s) 0.4095 Bahar / Arabic ✓
40000 40% (4m 41s) 0.8537 Armbruster / German ✓
45000 45% (5m 13s) 0.4365 Dritsas / Greek ✓
50000 50% (5m 47s) 1.6509 Fausti / Japanese ✗ (Italian)
55000 55% (6m 24s) 1.6823 Prince / French ✗ (English)
60000 60% (6m 54s) 0.9210 You / Korean ✓
65000 65% (7m 30s) 0.1240 Paraskevopoulos / Greek ✓
70000 70% (8m 3s) 1.5196 Victors / Dutch ✗ (French)
75000 75% (8m 48s) 0.1638 Morishita / Japanese ✓
80000 80% (9m 38s) 1.1936 Marshall / Scottish ✓
85000 85% (10m 26s) 1.3871 St pierre / Dutch ✗ (French)
90000 90% (11m 4s) 1.3021 Klerks / Czech ✗ (Dutch)
95000 95% (11m 42s) 1.2487 Joe / Korean ✗ (Chinese)
100000 100% (12m 23s) 0.0406 Nomikos / Greek ✓
```

- 损失对比曲线:

<img src="2.RNN%E7%BB%8F%E5%85%B8%E5%AE%9E%E4%BE%8B.assets/loss_compare.png" alt="loss_compare" style="zoom:50%;" />

- 损失对比曲线分析:

    模型训练的损失降低快慢代表模型收敛程度, 由图可知, 传统 RNN 的模型收敛情况最好, 然后是 GRU, 最后是 LSTM, 这是因为: 我们当前处理的文本数据是人名, 他们的长度有限, 且长距离字母间基本无特定关联, 因此无法发挥改进模型 LSTM 和 GRU 的长距离捕捉语义关联的优势. 所以在以后的模型选用时, 要通过对任务的分析以及实验对比, 选择最适合的模型.

- 训练耗时对比图:

<img src="2.RNN%E7%BB%8F%E5%85%B8%E5%AE%9E%E4%BE%8B.assets/period_compare.png" alt="period_compare" style="zoom:50%;" />

- 训练耗时对比图分析:

    模型训练的耗时长短代表模型的计算复杂度, 由图可知, 也正如我们之前的理论分析, 传统 RNN 复杂度最低, 耗时几乎只是后两者的一半, 然后是 GRU, 最后是复杂度最高的 LSTM.

- 结论:

    模型选用一般应通过实验对比, 并非越复杂或越先进的模型表现越好, 而是需要结合自己的特定任务, 从对数据的分析和实验结果中获得最佳答案.

5. **构建评估函数并进行预测**

- 构建传统 RNN 评估函数:

```python
def evaluateRNN(line_tensor):
    """评估函数, 和训练函数逻辑相同, 参数是line_tensor代表名字的张量表示"""
    # 初始化隐层张量
    hidden = rnn.initHidden()
    # 将评估数据line_tensor的每个字符逐个传入rnn之中
    for i in range(line_tensor.size()[0]):
        output, hidden = rnn(line_tensor[i], hidden)
    # 获得输出结果
    return output.squeeze(0)
```

- 构建 LSTM 评估函数:

```python
def evaluateLSTM(line_tensor):
    # 初始化隐层张量和细胞状态张量
    hidden, c = lstm.initHiddenAndC()
    # 将评估数据line_tensor的每个字符逐个传入lstm之中
    for i in range(line_tensor.size()[0]):
        output, hidden, c = lstm(line_tensor[i], hidden, c)
    return output.squeeze(0)
```

- 构建 GRU 评估函数:

```python
def evaluateGRU(line_tensor):
    hidden = gru.initHidden()
    # 将评估数据line_tensor的每个字符逐个传入gru之中
    for i in range(line_tensor.size()[0]):
        output, hidden = gru(line_tensor[i], hidden)
    return output.squeeze(0)
```

```python
line = "Bai"
line_tensor = lineToTensor(line)

rnn_output = evaluateRNN(line_tensor)
lstm_output = evaluateLSTM(line_tensor)
gru_output = evaluateGRU(line_tensor)
print("rnn_output:", rnn_output)
print("gru_output:", lstm_output)
print("gru_output:", gru_output)
```

- 输出效果:

```python
rnn_output: tensor([[ -1.9249,  -0.5806, -10.0495, -11.2049, -10.8971, -10.8805, -11.2915,
          -8.5678, -12.3079,  -5.7011,  -4.3192,  -5.5982,  -7.7489,  -9.0511,
         -12.1627,  -9.8225,  -7.6161,  -1.2990]], grad_fn=<SqueezeBackward1>)
lstm_output: tensor([[-3.1138, -0.8122, -6.6638, -8.2457, -7.8879, -7.5805, -7.1689, -7.4117,
         -6.8273, -3.6333, -3.0899, -2.0679, -5.7387, -6.4969, -7.2838, -6.9813,
         -5.5543, -1.2100]], grad_fn=<SqueezeBackward1>)
gru_output: tensor([[-3.8960, -0.5611, -8.1599, -8.7711, -8.9603, -8.7828, -7.6044, -8.3093,
         -8.8906, -4.1995, -4.1245, -2.3260, -6.7919, -8.0995, -8.1003, -7.5824,
         -6.5754, -1.2910]], grad_fn=<SqueezeBackward1>)
```

---

- 构建预测函数:

```python
def predict(input_line, evaluate, n_predictions=3):
    """
    input_line:		输入的字符串名字
    evaluate:		评估的模型函数, RNN, LSTM, GRU
    n_predictions:	需要取最有可能的top个结果
    """
    # 首先打印输入
    print('\n> %s' % input_line)

    # 以下操作的相关张量不进行求梯度
    with torch.no_grad():
        # 使输入的名字转换为张量表示, 并使用evaluate函数获得预测输出
        output = evaluate(lineToTensor(input_line))

        # 从预测的输出中取前3个最大的值及其索引
        topv, topi = output.topk(n_predictions, 1, True)
        # 创建盛装结果的列表
        predictions = []
        # 遍历n_predictions
        for i in range(n_predictions):
            # 从topv中取出的output值
            value = topv[0][i].item()
            # 取出索引并找到对应的类别
            category_index = topi[0][i].item()
            # 打印ouput的值, 和对应的类别
            print('(%.2f) %s' % (value, all_categories[category_index]))
            # 将结果装进predictions中
            predictions.append([value, all_categories[category_index]])
    return predictions
```

```python
for evaluate_fn in [evaluateRNN, evaluateLSTM, evaluateGRU]: 
    print("-"*18)
    predict('Dovesky', evaluate_fn)
    predict('Jackson', evaluate_fn)
    predict('Satoshi', evaluate_fn)
```

- 输出效果

```python
------------------

> Dovesky
(-0.57) Russian
(-1.08) Czech
(-2.87) English

> Jackson
(-0.06) Scottish
(-3.22) English
(-5.03) German

> Satoshi
(-0.76) Japanese
(-0.91) Italian
(-2.48) Greek
------------------

> Dovesky
(-0.85) Russian
(-1.64) Czech
(-2.23) Irish

> Jackson
(-0.34) Scottish
(-2.13) English
(-3.20) Czech

> Satoshi
(-0.51) Japanese
(-1.06) Arabic
(-3.45) Italian
------------------

> Dovesky
(-0.70) Russian
(-1.13) Czech
(-2.62) Polish

> Jackson
(-0.26) Scottish
(-2.20) English
(-3.43) Russian

> Satoshi
(-0.91) Arabic
(-0.95) Japanese
(-1.90) Italian
```

## 2.2 使用 seq2seq 模型架构实现英译法任务

- seq2seq 模型架构:

![img](2.RNN%E7%BB%8F%E5%85%B8%E5%AE%9E%E4%BE%8B.assets/s2s.png)

- seq2seq 模型架构分析:
  
    从图中可知, seq2seq 模型架构, 包括两部分分别是 encoder (编码器) 和 decoder (解码器), 编码器和解码器的内部实现都使用了 GRU 模型, 这里它要完成的是一个中文到英文的翻译: 欢迎 来 北京 –> welcome to BeiJing. 编码器首先处理中文输入 "欢迎 来 北京", 通过 GRU 模型获得每个时间步的输出张量, 最后将它们拼接成一个中间语义张量 $c$, 接着解码器将使用这个中间语义张量 $c$ 以及每一个时间步的隐层张量, 逐个生成对应的翻译语言.

- 翻译数据集:

    下载地址: https://download.pytorch.org/tutorial/data.zip

- 数据文件预览:

```
- data/
        - eng-fra.txt  
She feeds her dog a meat-free diet. Elle fait suivre à son chien un régime sans viande.
She feeds her dog a meat-free diet. Elle fait suivre à son chien un régime non carné.
She folded her handkerchief neatly. Elle plia soigneusement son mouchoir.
She folded her handkerchief neatly. Elle a soigneusement plié son mouchoir.
She found a need and she filled it. Elle trouva un besoin et le remplit.
She gave birth to twins a week ago. Elle a donné naissance à des jumeaux il y a une semaine.
She gave him money as well as food. Elle lui donna de l'argent aussi bien que de la nourriture.
She gave it her personal attention. Elle y a prêté son attention personnelle.
She gave me a smile of recognition. Elle m'adressa un sourire indiquant qu'elle me reconnaissait.
She glanced shyly at the young man. Elle a timidement jeté un regard au jeune homme.
She goes to the movies once a week. Elle va au cinéma une fois par semaine.
She got into the car and drove off. Elle s'introduisit dans la voiture et partit.
```

---

- 基于 GRU 的 seq2seq 模型架构实现翻译的过程:

    1. 导入必备的工具包.

    2. 对持久化文件中数据进行处理, 以满足模型训练要求.

    3. 构建基于 GRU 的编码器和解码器.

    4. 构建模型训练函数, 并进行训练.

    5. 构建模型评估函数, 并进行测试以及 Attention 效果分析.

1. **导入必备的工具包**

```python
pip install torch==1.3.1
# 从io工具包导入open方法
from io import open
# 用于字符规范化
import unicodedata
# 用于正则表达式
import re
# 用于随机生成数据
import random
# 用于构建网络结构和函数的torch工具包
import torch
import torch.nn as nn
import torch.nn.functional as F
# torch中预定义的优化方法工具包
from torch import optim
# 设备选择, 我们可以选择在cuda或者cpu上运行你的代码
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

2. **对持久化文件中数据进行处理, 以满足模型训练要求**

- 将指定语言中的词汇映射成数值:

```python
# 起始标志
SOS_token = 0
# 结束标志
EOS_token = 1

class Lang():
    def __init__(self, name):
        """
        name: 所传入某种语言的名字
        """
        self.name = name
        # 初始化单词到索引的映射字典
        self.word2index = {}
        # 初始化索引到单词的映射字典, 其中0, 1对应的SOS和EOS已经在字典中了
        self.index2word = {0: "SOS", 1: "EOS"}
        # 初始化词汇对应的数字索引, 从2开始, 因为0, 1已经被开始和结束标志占用了
        self.n_words = 2  

    def addSentence(self, sentence):
        """添加句子函数, 将整个句子中所有的单词依次添加到字典中"""
        # 根据一般国家的语言特性(我们这里研究的语言都是以空格分个单词), 直接进行分词就可以
        for word in sentence.split(' '):
            self.addWord(word)

    def addWord(self, word):
        """添加词汇函数, 即将词汇转化为对应的数值, 输入参数word是一个单词"""
        # 首先判断word是否已经在self.word2index字典的key中
        if word not in self.word2index:
            # 如果不在, 则将这个词加入其中, 并为它对应一个数值, 即self.n_words
            self.word2index[word] = self.n_words
            # 同时也将它的反转形式加入到self.index2word中
            self.index2word[self.n_words] = word
            # self.n_words一旦被占用之后, 逐次加1, 变成新的self.n_words
            self.n_words += 1
```

- 调用:

```python
name = "eng"
sentence = "hello I am Jay"

engl = Lang(name)
engl.addSentence(sentence)
print("word2index:", engl.word2index)
print("index2word:", engl.index2word)
print("n_words:", engl.n_words)
```

- 输出效果:

```python
word2index: {'hello': 2, 'I': 3, 'am': 4, 'Jay': 5}
index2word: {0: 'SOS', 1: 'EOS', 2: 'hello', 3: 'I', 4: 'am', 5: 'Jay'}
n_words: 6
```

---

- 字符规范化:

```python
# 将unicode转为Ascii, 去掉一些语言中的重音标记
def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn'
    )

def normalizeString(s):
    """字符串规范化函数, 参数s代表传入的字符串"""
    # 使字符变为小写并去除两侧空白符, z再使用unicodeToAscii去掉重音标记
    s = unicodeToAscii(s.lower().strip())
    # 在.!?前加一个空格
    s = re.sub(r"([.!?])", r" \1", s)
    # 使用正则表达式将字符串中不是大小写字母和正常标点的都替换成空格
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s
```

- 调用:

```python
s = "Are you kidding me?"
res = normalizeString(s)
print(res)
```

- 输出效果:

```shell
are you kidding me ?
```

---

- 将持久化文件中的数据加载到内存, 并实例化类 Lang

```python
data_path = './data/eng-fra.txt'

def readLangs(lang1, lang2):
    """
    读取语言函数
    lang1: 源语言的名字
    lang2: 目标语言的名字
    返回对应的class Lang对象, 以及语言对列表
    """
    # 从文件中读取语言对并以/n划分存到列表lines中
    lines = open(data_path, encoding='utf-8').read().strip().split('\n')
    # 对lines列表中的句子进行标准化处理, 并以\t进行再次划分, 形成子列表, 也就是语言对
    pairs = [[normalizeString(s) for s in l.split('\t')] for l in lines] 
    # 然后分别将语言名字传入Lang类中, 获得对应的语言对象, 返回结果
    input_lang = Lang(lang1)
    output_lang = Lang(lang2)
    return input_lang, output_lang, pairs
```

- 调用:

```python
lang1 = "eng"
lang2 = "fra"

input_lang, output_lang, pairs = readLangs(lang1, lang2)
print("input_lang:", input_lang)
print("output_lang:", output_lang)
print("pairs中的前五个:", pairs[:5])
```

- 输出效果:

```python
input_lang: <__main__.Lang object at 0x11ecf0828>
output_lang: <__main__.Lang object at 0x12d420d68>
pairs中的前五个: [['go .', 'va !'], ['run !', 'cours !'], ['run !', 'courez !'], ['wow !', 'ca alors !'], ['fire !', 'au feu !']]
```

---

- 过滤出符合我们要求的语言对:

```python
# 设置组成句子中单词或标点的最多个数
MAX_LENGTH = 10

# 选择带有指定前缀的语言特征数据作为训练数据
eng_prefixes = (
    "i am ", "i m ",
    "he is", "he s ",
    "she is", "she s ",
    "you are", "you re ",
    "we are", "we re ",
    "they are", "they re "
)

def filterPair(pair):
    """
    语言对过滤函数
    pair: 输入的语言对, 如['she is afraid.', 'elle malade.']
    """
    # pair[0]代表英文源语句, 它的长度应小于最大长度MAX_LENGTH并且要以指定的前缀开头
    # pair[1]代表法文源语句, 它的长度应小于最大长度MAX_LENGTH
    return len(pair[0].split(' ')) < MAX_LENGTH and pair[0].startswith(eng_prefixes) and len(pair[1].split(' ')) < MAX_LENGTH 

def filterPairs(pairs):
    """
    对多个语言对列表进行过滤
    pairs: 语言对组成的列表, 简称语言对列表
    """
    # 函数中直接遍历列表中的每个语言对并调用filterPair()即可
    return [pair for pair in pairs if filterPair(pair)]
```

- 调用:

```python
# 输入参数pairs使用readLangs函数的输出结果pairs
fpairs = filterPairs(pairs)
print("过滤后的pairs前五个:", fpairs[:5])
```

- 输出效果:

```python
过滤后的pairs前五个: [['i m .', 'j ai ans .'], ['i m ok .', 'je vais bien .'], ['i m ok .', 'ca va .'], ['i m fat .', 'je suis gras .'], ['i m fat .', 'je suis gros .']]
```

---

- 对以上数据准备函数进行整合, 并使用类 Lang 对语言对进行数值映射:

```python
def prepareData(lang1, lang2):
    """
    数据准备函数, 作用是将所有字符串数据向数值型数据的映射以及过滤语言对
    lang1, lang2分别代表源语言和目标语言的名字
    """
    input_lang, output_lang, pairs = readLangs(lang1, lang2)
    pairs = filterPairs(pairs)
    for pair in pairs:
        input_lang.addSentence(pair[0])
        output_lang.addSentence(pair[1])
    return input_lang, output_lang, pairs
```

- 调用:

```python
input_lang, output_lang, pairs = prepareData('eng', 'fra')
print("input_n_words:", input_lang.n_words)
print("output_n_words:", output_lang.n_words)
print(random.choice(pairs))
```

- 输出效果:

```python
input_n_words: 2803
output_n_words: 4345
pairs随机选择一条: ['you re such an idiot !', 'quelle idiote tu es !']
```

---

- 将语言对转化为模型输入需要的张量:

```python
def tensorFromSentence(lang, sentence):
    """
    将句子文本转换为张量
    lang:     Lang的实例化对象
    sentence: 预转换的句子
    """
    # 遍历句子中的每一个词汇, 并取得其在lang.word2index()中对应的索引
    indexes = [lang.word2index[word] for word in sentence.split(' ')]
    # 加入句子结束标志
    indexes.append(EOS_token)
    # 使用torch.tensor封装成n*1张量, 方便后续计算
    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)

def tensorsFromPair(pair):
    """将语言对转换为张量对"""
    input_tensor = tensorFromSentence(input_lang, pair[0])
    target_tensor = tensorFromSentence(output_lang, pair[1])
    return (input_tensor, target_tensor)
```

- 调用:

```python
pair = pairs[0]

pair_tensor = tensorsFromPair(pair)
print(pair_tensor)
```

- 输出效果:

```python
tensor([[2],
        [3],
        [4],
        [1]], device='cuda:0'),
tensor([[2],
        [3],
        [4],
        [5],
        [1]], device='cuda:0')
```

3. **构建基于 GRU 的编码器和解码器**

- 构建基于 GRU 的编码器

    编码器结构图:

![img](2.RNN%E7%BB%8F%E5%85%B8%E5%AE%9E%E4%BE%8B.assets/encoder-network.png)

```python
class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        """
        input_size: 解码器的输入尺寸 (源语言词表大小)
        hidden_size: 隐层节点数. 即GRU层的输入尺寸 或 词嵌入维度
        """
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size
        # 实例化nn中预定义的Embedding层, input_size为源语言词表大小,
        # hidden_size为源语言词嵌入维度
        self.embedding = nn.Embedding(input_size, hidden_size).to(device)
        # 实例化nn中预定义的GRU层, 由于上层的输出维度是hidden_size
        # 因此GRU层的输入维度也是hidden_size
        self.gru = nn.GRU(hidden_size, hidden_size).to(device)

    def forward(self, x, hidden):
        """
        x:      源语言的Embedding层输入张量
        hidden: 编码器gru层的初始隐层张量
        """
        # 理论上, 编码器每次只以一个词作为输入, embedding后的尺寸应该是 1*embedding_size
        # 而torch中预定义gru必须使用三维张量作为输入, 因此需要拓展一个维度
        output = self.embedding(x).view(1, 1, -1)
        output, hidden = self.gru(output, hidden)
        return output, hidden

    def initHidden(self):
        """初始化隐层张量函数"""
        return torch.zeros(1, 1, self.hidden_size, device=device)
```

- 调用:

```python
hidden_size = 25
input_size = 20

# pair_tensor[0]为源语言即英文的句子, pair_tensor[0][0]为句子中的第一个词
x = pair_tensor[0][0]
# 按1*1*hidden_size的全0张量初始化第一个隐层张量
hidden = torch.zeros(1, 1, hidden_size, device=device)

encoder = EncoderRNN(input_size, hidden_size)
encoder_output, hidden = encoder(x, hidden)
print(encoder_output)
print(encoder_output.shape)
```

- 输出效果:

```python
tensor([[[-0.1397,  0.0109, -0.1849, -0.3340, -0.0594, -0.1913, -0.4319,
           0.1061,  0.4396,  0.0940, -0.0847,  0.4061,  0.0768, -0.2013,
           0.2623, -0.2501, -0.2133, -0.0313, -0.0617, -0.2441, -0.0423,
           0.1314,  0.4308, -0.1657, -0.1865]]], device='cuda:0',
       grad_fn=<CudnnRnnBackward>)
torch.Size([1, 1, 25])
```

---

- 构建基于 GRU 的解码器

    解码器结构图:

![decoder-network](2.RNN%E7%BB%8F%E5%85%B8%E5%AE%9E%E4%BE%8B.assets/decoder-network.png)

```python
class DecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        """
        hidden_size: 隐层节点数. 即GRU层的输入尺寸
        output_size: 解码器的输出尺寸 (目标语言词表大小)
        """
        super(DecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        # 实例化nn中预定义的Embedding层, output_size为目标语言的词表大小
        # hidden_size为目标语言词嵌入维度
        self.embedding = nn.Embedding(output_size, hidden_size)
        # 实例化nn中预定义的GRU层, 由于上层的输出维度是hidden_size
        # 因此GRU层的输入维度也是hidden_size
        self.gru = nn.GRU(hidden_size, hidden_size)
        # 实例化线性层, output_size为希望的输出尺寸
        self.out = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, x, hidden):
        """
        x:      目标语言的Embedding层输入张量
        hidden: 解码器GRU层的初始隐层张量
        """
        # 对输入张量进行embedding操作
        output = self.embedding(x).view(1, 1, -1)
        # 根据relu函数的特性, 将使Embedding矩阵更稀疏, 以防止过拟合
        output = F.relu(output)
        output, hidden = self.gru(output, hidden)
        # GRU层的输出output为三维张量, 第一维没有意义, 可以通过output[0]来降维
        # 再经线性层变换, 传入softmax层处理以便于分类
        output = self.softmax(self.out(output[0]))
        return output, hidden

    def initHidden(self):
        """初始化隐层张量函数"""
        return torch.zeros(1, 1, self.hidden_size, device=device)
```

- 调用:

```python
hidden_size = 25
output_size = 10

# pair_tensor[1]为目标语言即法文的句子, pair_tensor[1][0]为句子中的第一个词
x = pair_tensor[1][0]
# 按1*1*hidden_size的全0张量初始化第一个隐层张量
hidden = torch.zeros(1, 1, hidden_size)

decoder = DecoderRNN(hidden_size, output_size)
output, hidden = decoder(x, hidden)
print(output)
print(output.shape)
```

- 输出效果:

```python
tensor([[-2.1477, -2.5909, -2.2206, -2.5508, -2.4880, -2.2687, -2.4027,
-2.0865,
         -2.4021, -2.0395]], device='cuda:0', grad_fn=<LogSoftmaxBackwar
d>)
torch.Size([1, 10])
```

---

- 构建基于 GRU 和 Attention 的解码器

    解码器结构图:

<img src="2.RNN%E7%BB%8F%E5%85%B8%E5%AE%9E%E4%BE%8B.assets/attention-decoder-network.png" alt="attention-decoder-network" style="zoom:67%;" />

```python
class AttnDecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):
        """
        hidden_size: 隐层节点数. 即GRU层的输入尺寸
        output_size: 解码器的输出尺寸 (目标语言词表大小)
        dropout_p:   置零比率, 默认0.1
        max_length:  句子的最大长度
        """
        super(AttnDecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.dropout_p = dropout_p
        self.max_length = max_length

        # 实例化nn中预定义的Embedding层
        self.embedding = nn.Embedding(output_size, hidden_size).to(device)

        # 根据attention的QKV理论, attention的输入参数为三个, Q, K, V
        # ### 第一步 ###
        # 使用Q与K进行attention权值计算得到权重矩阵, 再与V做矩阵乘法, 得到V的注意力表示结果
        # 常见的计算方式有三种:
        # 1. 将Q, K进行纵轴拼接, 做一次线性变化, 最后使用softmax处理获得结果最后与V做张量乘法
        # 2. 将Q, K进行纵轴拼接, 做一次线性变化后使用tanh函数激活, 然后进行内部求和, 最后使用softmax处理获得结果再与V做张量乘法
        # 3. 将Q与K的转置做点积运算, 然后除以一个缩放系数, 最后使用softmax处理获得结果最后与V做张量乘法
        # 说明：当注意力权重矩阵和V都是三维张量且第一维代表为batch条数时, 则做bmm运算
        # ### 第二步 ###
        # 根据第一步采用的计算方法, 如果是拼接方法, 则需要将Q与第一步的计算结果再进行拼接
        # 如果是转置点积, 一般是自注意力 (Q与V相同), 则不需要进行与Q的拼接. 因此第二步的计算方式与第一步采用的全值计算方法有关
        # ### 第三步 ###
        # 最后为了使整个attention结构按照指定尺寸输出, 使用线性层在第二步的结果上做一个线性变换, 得到最终对Q的注意力表示

        # 这里使用第一种计算方式, 因此需要实例化一个线性变换的矩阵
        # 由于输入是Q, K的纵轴拼接, 所以nn.Linear()的第一个参数为hidden_size*2, 第二个参数为max_length
        # Q为解码器Embedding层的输出. K为解码器GRU的隐层输出, 因为首次隐层还没有任何输出, 会使用编码器的隐层输出
        # V为编码器层的输出
        self.attn = nn.Linear(hidden_size*2, max_length).to(device)
        # 实例化nn.Dropout层, 并传入self.dropout_p
        self.dropout = nn.Dropout(dropout_p).to(device)

        # 实例化另外一个线性层, 用于规范输出尺寸, 其输入来自第三步的结果 (将Q与第二步的结果进行拼接)
        # 输入维度是self.hidden_size*2
        self.attn_combine = nn.Linear(hidden_size*2, hidden_size).to(device)
        # 实例化nn.GRU层, 它的输入和隐层尺寸都是self.hidden_size
        self.gru = nn.GRU(hidden_size, hidden_size).to(device)
        # 实例化gru后面的线性层, 也即解码器输出层
        self.out = nn.Linear(hidden_size, output_size).to(device)

    def forward(self, x, hidden, encoder_outputs):
        """
        x:      目标语言的Embedding层输入张量, [1]
        hidden: 解码器GRU层的初始隐层张量, 1*1*hidden_size
        encoder_outputs: 解码器的输出张量, max_length*hidden_size
        """
        # 对输入张量进行embedding操作
        # 1 -> hidden_size -> 1*1*hidden_size
        embedded = self.embedding(x).view(1, 1, -1)
        # 使用dropout进行随机丢弃, 防止过拟合
        embedded = self.dropout(embedded)

        # ### 第一步 ###
        # 使用第一种计算方式进行attention的权重计算
        # 将Q, K进行纵轴拼接, 并做一次线性变化, 最后使用softmax处理获得结果
        # attn_weights: 1*hidden_size cat 1*hidden_size -> 1*2xhidden_size -> 1*max_length
        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=-1)
        # 将得到的权重矩阵与V做矩阵乘法计算, 当二者都是三维张量且第一维代表为batch条数时, 则做bmm运算
        # 1*1*max_length bmm 1*max_length*hidden_size -> 1*1*hidden_size
        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))

        # ### 第二步 ###
        # 通过取[0]降维, 根据第一步采用的计算方法, 将Q与第一步的计算结果再进行拼接
        # 1*hidden_size cat 1*hidden_size -> 1*2xhidden_size
        output = torch.cat((embedded[0], attn_applied[0]), 1)

        # ### 第三步 ###
        # 在第二步的输出结果上做一个线性变换并扩展维度, 得到输出
        # 1*2xhidden_size -> 1*hidden_size -> 1*1*hidden_size
        output = self.attn_combine(output).unsqueeze(0)

        # attention结构的结果使用relu激活
        output = F.relu(output)
        # 将激活后的结果作为gru的输入和hidden一起传入其中
        output, hidden = self.gru(output, hidden)
        # 将结果降维并使用softmax处理得到最终的结果
        # 1*1*hidden_size -> 1*hidden_size -> 1*output_size
        output = F.log_softmax(self.out(output[0]), dim=-1)

        # 返回解码器结果, 最后的隐层张量以及注意力权重张量
        return output, hidden, attn_weights

    def iniHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)
```

- 调用:

```python
hidden_size = 25
output_size = 10

x = pair_tensor[1][0]
hidden = torch.zeros(1, 1, hidden_size)
# encoder_outputs需要是encoder中每一个时间步的输出堆叠而成
# 它的形状应该是max_length*hidden_size, 10*25
encoder_outputs  = torch.randn(10, 25)

decoder = AttnDecoderRNN(hidden_size, output_size)
output, hidden, attn_weights= decoder(x, hidden, encoder_outputs)
print(output)
print(output.shape)
print(hidden.shape)
print(attn_weights.shape)
```

- 输出效果:

```python
tensor([[-2.1785, -2.4324, -2.5268, -2.3792, -1.9043, -2.2876, -2.2034,
-2.3756,
         -2.3938, -2.5095]], device='cuda:0', grad_fn=<LogSoftmaxBackwar
d>)
torch.Size([1, 10])
torch.Size([1, 1, 25])
torch.Size([1, 10])
```

4. **构建模型训练函数, 并进行训练**

- 什么是 teacher_forcing?

    它是一种用于序列生成任务的训练技巧, 在 seq2seq 架构中, 根据循环神经网络理论, 解码器每次应该使用上一步的结果作为输入的一部分, 但是训练过程中, 一旦上一步的结果是错误的, 就会导致这种错误被累积, 无法达到训练效果, 因此, 我们需要一种机制改变上一步出错的情况, 因为训练时我们是已知正确的输出应该是什么, 因此可以强制将上一步结果设置成正确的输出, 这种方式就叫做 teacher_forcing.

- teacher_forcing 的作用:

    能够在训练的时候矫正模型的预测, 避免在序列生成的过程中误差进一步放大.

    teacher_forcing 能够极大的加快模型的收敛速度, 令模型训练过程更快更平稳.

- 构建训练函数:

```python
teacher_forcing_ratio = 0.5
def train(input_tensor, target_tensor, encoder, decoder,
        encoder_optimizer, decoder_optimizer, criterion,
        max_length=MAX_LENGTH):
    """
    input_tensor:      源语言输入张量
    target_tensor:     目标语言输入张量
    encoder, decoder:  编码器和解码器实例化对象
    encoder_optimizer: 编码器优化方法
    decoder_optimizer: 解码器优化方法
    criterion:         损失函数计算方法
    max_length:        句子的最大长度
    """
    # 初始化隐层张量
    encoder_hidden = encoder.initHidden()
    # 编码器和解码器的优化器梯度归0
    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()
    # 根据源文本和目标文本张量获得对应的长度
    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)
    # 以max_length*encoder.hidden_size的全0张量初始化编码器输出张量
    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)
    # 初始设置损失
    # loss = 0
    loss = criterion(torch.tensor([[0.]], device=device), torch.tensor([0], device=device))

    for ei in range(input_length):
        # 根据索引从input_tensor取出对应单词的张量表示, 和初始化隐层张量一同传入encoder对象中
        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)
        # encoder_output为三维张量, 使用[0]降维变成向量依次存入到encoder_outputs
        # encoder_outputs的每一行为对应的句子中每个单词通过编码器的输出结果 (1*1*encoder.hidden_size)
        encoder_outputs[ei] = encoder_output[0]

    # 初始化解码器的第一个输入, 即起始符
    decoder_input = torch.tensor([SOS_token], device=device)
    # 初始化解码器的隐层张量即编码器的隐层输出
    decoder_hidden = encoder_hidden
    # 根据随机数与teacher_forcing_ratio对比判断是否使用teacher_forcing
    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False

    if use_teacher_forcing:
        # 循环遍历目标张量索引
        for di in range(target_length):
            # 将decoder_input, decoder_hidden, encoder_outputs即attention中的QKV传入解码器对象
            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)
            # 由于使用了teacher_forcing, 无论解码器输出的decoder_output是什么
            # 都只使用 '正确答案', 即target_tensor[di]来计算损失
            loss += criterion(decoder_output, target_tensor[di])
            decoder_input = target_tensor[di]
    else:
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)
            # 从decoder_output取出答案
            topv, topi = decoder_output.topk(1)
            # 损失计算仍然使用decoder_output和target_tensor[di]
            loss += criterion(decoder_output, target_tensor[di])
            # 如果输出值是终止符, 则循环停止
            if topi.squeeze().item() == EOS_token:
                break
            # 否则, 对topi降维并分离赋值给decoder_input以便进行下次运算
            # detach的分离作用使得这个decoder_input与模型构建的张量图无关, 相当于全新的外界输入
            decoder_input = topi.squeeze().detach()

    loss.backward()
    encoder_optimizer.step()
    decoder_optimizer.step()

    # 返回平均损失
    return loss.item() / target_length
```

---

- 构建时间计算函数:

```python
# 时间和数学工具包
import time
import math

def timeSince(since):
    "获得每次打印的训练耗时, since是训练开始时间"
    # 获得当前时间
    now = time.time()
    # 获得时间差, 就是训练耗时
    s = now - since
    # 将秒转化为分钟, 并取整
    m = math.floor(s / 60)
    # 计算剩下不够凑成1分钟的秒数
    s -= m * 60
    # 返回指定格式的耗时
    return '%dm %ds' % (m, s)
```

- 调用:

```python
# 假定模型训练开始时间是10min之前
since = time.time() - 10*60

period = timeSince(since)
print(period)
```

- 输出效果:

```python
10m 0s 
```

---

- 调用训练函数并打印日志和制图:

```python
# 导入plt以便绘制损失曲线
import matplotlib.pyplot as plt

def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):
    """
    训练迭代函数
    encoder, decoder: 编码器和解码器对象
    n_iters:          总迭代步数
    print_every:      打印日志间隔
    plot_every:       绘制损失曲线间隔
    learning_rate:    学习率
    """
    ENCODER_PATH = './models/translator_eng2fra_encoder.pth'
    DECODER_PATH = './models/translator_eng2fra_decoder.pth'
    if not (os.path.exists(ENCODER_PATH) and os.path.exists(DECODER_PATH)):
        # 获得训练开始时间戳
        start = time.time()
        # 每个损失间隔的平均损失保存列表, 用于绘制损失曲线
        plot_losses = []
        # 每个打印日志间隔的总损失, 初始为0
        print_loss_total = 0
        # 每个绘制损失间隔的总损失, 初始为0
        plot_loss_total = 0

        # 使用预定义的SGD作为优化器, 将参数和学习率传入其中
        encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
        decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)

        # 选择损失函数
        criterion = nn.NLLLoss()

        for iter in range(1, n_iters+1):
            # 每次从语言对列表中随机取出一条作为训练语句
            training_pair = tensorsFromPair(random.choice(pairs))
            # 分别从training_pair中取出输入张量和目标张量
            input_tensor = training_pair[0]
            target_tensor = training_pair[1]

            # 通过train函数获得模型运行的损失
            loss = train(input_tensor, target_tensor, encoder, decoder,
                    encoder_optimizer, decoder_optimizer,
                    criterion)
            # 将损失进行累和
            print_loss_total += loss
            plot_loss_total = loss

            if iter % print_every == 0:
                # 通过总损失除以间隔得到平均损失
                print_loss_avg = print_loss_total / print_every
                # 总损失归0
                print_loss_total = 0
                # 打印日志, 日志内容分别是: 训练耗时, 当前迭代步, 当前进度百分比, 当前平均损失
                print('%s (%d %d%%) %.4f' % (timeSince(start),
                    iter, iter / n_iters * 100, print_loss_avg))

            if iter % plot_every == 0:
                # 通过总损失除以间隔得到平均损失
                plot_loss_avg = plot_loss_total / plot_every
                # 将平均损失装进plot_losses列表
                plot_losses.append(plot_loss_avg)
                # 总损失归0
                plot_loss_total = 0

        torch.save(encoder.state_dict(), ENCODER_PATH)
        torch.save(attn_decoder.state_dict(), DECODER_PATH)

        # 绘制损失曲线
        plt.figure()
        plt.plot(plot_losses)
        # 保存到指定路径
        plt.savefig("./figures/s2s_loss.png")
    else:
        encoder.load_state_dict(torch.load(ENCODER_PATH))
        attn_decoder.load_state_dict(torch.load(DECODER_PATH))
```

- 调用:

```python
# 设置隐层大小为256 , 也是词嵌入维度
hidden_size = 256
# 通过input_lang.n_words获取输入词汇总数
encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)
# 通过output_lang.n_words获取目标词汇总数
attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words,
        dropout_p=0.1).to(device)

n_iters = 75000
print_every = 5000

# 调用trainIters进行模型训练
trainIters(encoder, attn_decoder, n_iters, print_every=print_every)
```

- 输出效果:

```shell
1m 58s (5000 6%) 3.4148
3m 49s (10000 13%) 2.7672
5m 39s (15000 20%) 2.4162
7m 40s (20000 26%) 2.1866
9m 44s (25000 33%) 1.9829
11m 47s (30000 40%) 1.7851
13m 50s (35000 46%) 1.6492
15m 51s (40000 53%) 1.4993
17m 53s (45000 60%) 1.3651
19m 55s (50000 66%) 1.3185
22m 2s (55000 73%) 1.2237
24m 2s (60000 80%) 1.1185
26m 8s (65000 86%) 1.0594
28m 21s (70000 93%) 0.9930
30m 17s (75000 100%) 0.9572
```

- 损失下降曲线:

<img src="2.RNN%E7%BB%8F%E5%85%B8%E5%AE%9E%E4%BE%8B.assets/s2s_loss-16667733933051.png" alt="s2s_loss" style="zoom:50%;" />

- 损失下降曲线 (他人):<img src="2.RNN%E7%BB%8F%E5%85%B8%E5%AE%9E%E4%BE%8B.assets/s2s_loss.png" alt="s2s_loss" style="zoom:50%;" />

- 损失曲线分析:

    一直下降的损失曲线, 说明模型正在收敛, 能够从数据中找到一些规律应用于数据.

5. **构建模型评估函数, 并进行测试以及 Attention 效果分析.**

- 构建模型评估函数:

```python
def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):
    """
    评估函数
    encoder, decoder: 编码器和解码器对象,
    sentence:         需要评估的句子
    max_length:       句子的最大长度
    """
    # 评估阶段不进行梯度计算
    with torch.no_grad():
        # 对输入的句子进行张量表示
        input_tensor = tensorFromSentence(input_lang, sentence)
        # 获得输入的句子长度
        input_length = input_tensor.size(0)
        # 初始化编码器隐层张量
        encoder_hidden = encoder.initHidden()

        # 以max_length*encoder.hidden_size的全0张量初始化编码器输出张量
        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

        for ei in range(input_length):
            # 根据索引从input_tensor取出对应单词的张量表示, 和初始化隐层张量一同传入encoder对象中
            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)
            # encoder_output为三维张量, 使用[0]降两维变成向量依次存入到encoder_outputs
            # encoder_outputs的每一行为对应的句子中每个单词通过编码器的输出结果 (1*1*encoder.hidden_size)
            encoder_outputs[ei] = encoder_output[0]

        # 初始化解码器的第一个输入, 即起始符
        decoder_input = torch.tensor([SOS_token], device=device)
        # 初始化解码器的隐层张量即编码器的隐层输出
        decoder_hidden = encoder_hidden

        # 初始化预测的词汇列表
        decoded_words = []
        # 初始化attention张量
        decoder_attentions = torch.zeros(max_length, max_length)

        for di in range(max_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)
            # 取所有的attention结果存入初始化的attention张量中
            decoder_attentions[di] = decoder_attention.data
            # 从解码器输出中获得概率最高的值及其索引对象
            topv, topi = decoder_output.topk(1)
            # 从索引对象中取出它的值与结束标志值作对比
            if topi.item() == EOS_token:
                # 如果是结束标志值, 则将结束标志装进decoded_words列表, 代表翻译结束
                decoded_words.append('<EOS>')
                break
            else:
                # 否则, 根据索引找到它在输出语言的index2word字典中对应的单词装进decoded_words
                decoded_words.append(output_lang.index2word[topi.item()])
            # 最后将本次预测的索引降维并分离赋值给decoder_input, 以便下次进行预测
            decoder_input = topi.squeeze().detach()
        # 返回decoded_words, 以及完整注意力张量, 把没有用到的部分切掉
        return decoded_words, decoder_attentions[:di+1]
```

---

- 随机选择指定数量的数据进行评估:

```python
def evaluateRandomly(encoder, decoder, n=6):
    """随机测试函数"""
    for i in range(n):
        pair = random.choice(pairs)
        # > 代表输入
        print('>', pair[0])
        # = 代表正确的输出
        print('=', pair[1])
        # 调用evaluate进行预测
        output_words, attentions = evaluate(encoder, decoder, pair[0])
        # 将结果连成句子
        output_sentence = ' '.join(output_words)
        # < 代表模型的输出
        print('>', output_words)
        print('')
```

- 调用:

```python
evaluateRandomly(encoder1, attn_decoder1)
```

- 输出效果:

```python
> i m crazy about you .
= je suis fou de vous .
> ['je', 'suis', 'fou', 'de', 'toi', '.', '<EOS>']

> he s lived here his entire life .
= il a vecu ici toute sa vie .
> ['il', 'a', 'vecu', 'ici', 'toute', 'sa', 'vie', '.', '<EOS>']

> you re very brave .
= vous etes fort courageux .
> ['vous', 'etes', 'tres', 'courageux', '.', '<EOS>']

> i m sorry i yelled at you .
= je suis desolee de vous avoir hurle dessus .
> ['je', 'suis', 'desolee', 'de', 't', 'avoir', 'crie', 'apres', '.', '<EOS>']

> you are very insensitive .
= tu es tres indelicat .
> ['vous', 'etes', 'tres', 'indelicat', '.', '<EOS>']

> he s got lung cancer .
= il a le cancer des poumons .
> ['il', 'a', 'le', 'mauvais', 'de', 'la', '.', '.', '<EOS>']
```

---

- Attention 张量制图:

```python
sentence = "we re both teachers ."
output_words, attentions = evaluate(
encoder1, attn_decoder1, sentence)
print(output_words)
plt.matshow(attentions.numpy())
plt.savefig("./s2s_attn.png")
```

- 输出效果:

```shell
['nous', 'sommes', 'tous', 'deux', 'temoins', '.', '<EOS>']
```

- Attention 可视化:

<img src="2.RNN%E7%BB%8F%E5%85%B8%E5%AE%9E%E4%BE%8B.assets/s2s_attn.png" alt="s2s_attn" style="zoom:50%;" />

Attention 图像的纵坐标代表输入的源语言各个词汇对应的索引, 0-6 分别对应 `["we", "re", "both", "teachers", ".", ""]`, 纵坐标代表生成的目标语言各个词汇对应的索引, 0-7 代表 `["nous", "sommes", "toutes", "deux", "enseignantes", ".", ""]`, 图中浅色小方块 (颜色越浅说明影响越大) 代表词汇之间的影响关系, 比如源语言的第 1 个词汇对生成目标语言的第 1 个词汇影响最大, 源语言的第 4, 5 个词对生成目标语言的第 5 个词会影响最大, 通过这样的可视化图像, 我们可以知道 Attention 的效果好坏, 与我们人为去判定到底还有多大的差距, 进而衡量我们训练模型的可用性.