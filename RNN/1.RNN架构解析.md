# 1. RNN 架构解析

[toc]

## 1.1 认识 RNN 模型

- 什么是RNN模型:
  
    RNN (Recurrent Neural Network), 中文称作循环神经网络, 它一般以序列数据为输入, 通过网络内部的结构设计有效捕捉序列之间的关系特征, 一般也是以序列形式进行输出.

- RNN 单层网络结构:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN2.gif" alt="img" style="zoom: 67%;" />

- 以时间步对 RNN 进行展开后的单层网络结构:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN1.gif" alt="img" style="zoom: 50%;" />

> RNN 的循环机制使模型隐层上一时间步产生的结果, 能够作为当下时间步输入的一部分 (当下时间步的输入除了正常的输入外还包括上一步的隐层输出) 对当下时间步的输出产生影响.

- RNN 模型的作用:
  
    因为 RNN 结构能够很好利用序列之间的关系, 因此针对自然界具有连续性的输入序列, 如人类的语言, 语音等进行很好的处理, 广泛应用于 NLP 领域的各项任务, 如文本分类, 情感分析, 意图识别, 机器翻译等.

---

- 下面我们将以一个用户意图识别的例子进行简单的分析:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN3.gif" alt="img" style="zoom:67%;" />

1. 第一步：用户输入了"What time is it ?", 我们首先需要对它进行基本的分词, 因为 RNN 是按照顺序工作的, 每次只接收一个单词进行处理.

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN6.gif" alt="RNN6" style="zoom:67%;" />

2. 第二步：首先将单词 "What" 输送给 RNN, 它将产生一个输出 O1.

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN4.gif" alt="RNN4" style="zoom:67%;" />

------

3. 第三步：继续将单词 "time" 输送给 RNN, 但此时 RNN 不仅仅利用 "time" 来产生输出 O2, 还会使用来自上一层隐层输出 O1 作为输入信息.

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN7.gif" alt="RNN7" style="zoom:67%;" />

------

4. 第四步：重复这样的步骤, 直到处理完所有的单词.

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN8.gif" alt="RNN8" style="zoom:67%;" />

------

5. 第五步：最后, 将最终的隐层输出 O5 进行处理来解析用户意图.

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN9.gif" alt="RNN9" style="zoom:67%;" />

---

- RNN 模型的分类:
  
    这里我们将从两个角度对 RNN 模型进行分类. 第一个角度是输入和输出的结构, 第二个角度是 RNN 的内部构造.
    
- 按照输入和输出的结构进行分类:
    - N vs N - RNN
    - N vs 1 - RNN
    - 1 vs N - RNN
    - N vs M - RNN

- 按照 RNN 的内部构造进行分类:
    - 传统 RNN
    - LSTM
    - Bi-LSTM
    - GRU
    - Bi-GRU

1. N vs N - RNN:

    它是 RNN 最基础的结构形式, 最大的特点就是: 输入和输出序列是等长的. 由于这个限制的存在, 使其适用范围比较小, 可用于生成等长度的合辙诗句.

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/10.png" alt="img" style="zoom: 50%;" />

2. N vs 1 - RNN:

    有时候我们要处理的问题输入是一个序列, 而要求输出是一个单独的值而不是序列, 应该怎样建模呢? 我们只要在最后一个隐层输出 $h_4$ 上进行线性变换就可以了, 大部分情况下, 为了更好的明确结果, 还要使用 $Sigmoid$ 或者 $Softmax$ 进行处理。这种结构经常被应用在文本分类问题上.

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/11.png" alt="img" style="zoom:50%;" />

3. 1 vs N - RNN:

    如果输入不是序列而输出为序列的情况怎么处理呢? 我们最常采用的一种方式就是使该输入作用于每次的输出之上. 这种结构可用于将图片生成文字任务等.

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/12.png" alt="img" style="zoom:50%;" />

4. N vs M - RNN:

    这是一种不限输入输出长度的 RNN 结构, 它由编码器和解码器两部分组成, 两者的内部结构都是某类 RNN, 它也被称为 seq2seq 架构. 输入数据首先通过编码器, 最终输出一个隐含变量 $c$, 之后最常用的做法是使用这个隐含变量 $c$ 作用在解码器进行解码的每一步上, 以保证输入信息被有效利用.

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN15.png" alt="RNN15" style="zoom:50%;" />

> seq2seq 架构最早被提出应用于机器翻译, 因为其输入输出不受限制, 如今也是应用最广的 RNN 模型结构. 在机器翻译, 阅读理解, 文本摘要等众多领域都进行了非常多的应用实践.

## 1.2 传统 RNN 模型

- 传统 RNN 的内部结构图:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/21.png" alt="img" style="zoom:50%;" />

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/22.png" alt="img" style="zoom: 67%;" />

- 内部结构分析:

    我们把目光集中在中间的方块部分, 它的输入有两部分, 分别是 $h_{t-1}$ 以及 $x_t$, 代表上一时间步的隐层输出, 以及此时间步的输入, 它们进入 RNN 结构体后, 会"融合"到一起, 这种融合我们根据结构解释可知, 是将二者进行拼接, 形成新的张量 $[x_t, h_{t-1}]$, 之后这个新的张量将通过一个全连接层 (线性层), 该层使用 $tanh$ 作为激活函数, 最终得到该时间步的输出 $h_t$, 它将作为下一个时间步的输入和 $x_{t+1}$ 一起进入结构体, 以此类推.

- 内部结构过程演示:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN21.gif" alt="RNN21" style="zoom: 67%;" />

根据结构分析得出内部计算公式:
$$
h_t = tanh(W_t[X_t, h_{t-1}] + b_t)
$$

- 激活函数 $tanh$ 的作用:
  
    用于帮助调节流经网络的值, $tanh$ 函数将值压缩在 -1 和 1 之间.

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN22.gif" alt="RNN22" style="zoom:67%;" />

---

- Pytorch 中传统 RNN 工具的使用:
  
    位置: 在 `torch.nn` 工具包之中, 通过 `torch.nn.RNN` 可调用.

- `nn.RNN` 类初始化主要参数解释:
  
    - `input_size`: 输入张量 $x$ 中特征维度的大小.
    
    - `hidden_size`: 隐层张量 $h$ 中特征维度的大小.
    
    - `num_layers`: 隐含层的数量.
    
    - `nonlinearity`: 激活函数的选择, 默认是 $tanh$.

- `nn.RNN` 类实例化对象主要参数解释:
  
    - `input`: 输入张量 $x$.
    
    - `h0`: 初始化的隐层张量 $h$.

- `nn.RNN` 使用示例:

```python
import torch
import torch.nn as nn

# 实例化rnn对象
# input_size: 输入张量x的维度
# hidden_size: 隐藏层的维度，隐藏层神经元数量
# num_layers: 隐藏层的层数 
rnn = nn.RNN(5, 6, 1)

# 初始化输入张量x
# sequence_length: 输入序列的长度
# batch_size: 批次的样本数
# input_size: 输入张量x的维度
x = torch.randn(1, 3, 5)

# 初始化隐藏层张量h0
# num_layers * num_directions: 隐藏层的层数*网络方向数
# batch_size: 批次的样本数
# hidden_size: 隐藏层的维度
h0 = torch.randn(1, 3, 6)

# 将x输入rnn中, 得到输出张量结果
output, hn = rnn(x, h0)

print(output)
print(output.shape)
print(hn)
print(hn.shape)
```

```python
tensor([[[ 0.6781, -0.9230,  0.3058, -0.1920,  0.1668, -0.8229],
         [-0.3773,  0.7475, -0.5666, -0.2041, -0.2856, -0.7253],
         [-0.1128,  0.1839,  0.9041,  0.7925,  0.6267,  0.5288]]],
       grad_fn=<StackBackward>)
torch.Size([1, 3, 6])
tensor([[[ 0.6781, -0.9230,  0.3058, -0.1920,  0.1668, -0.8229],
         [-0.3773,  0.7475, -0.5666, -0.2041, -0.2856, -0.7253],
         [-0.1128,  0.1839,  0.9041,  0.7925,  0.6267,  0.5288]]],
       grad_fn=<StackBackward>)
torch.Size([1, 3, 6])
```

- 传统 RNN 的优势:

    由于内部结构简单, 对计算资源要求低, 相比之后我们要学习的 RNN 变体：LSTM 和 GRU 模型参数总量少了很多, 在短序列任务上性能和效果都表现优异.

- 传统 RNN 的缺点:

    传统 RNN 在解决长序列之间的关联时, 通过实践, 证明经典 RNN 表现很差, 原因是在进行反向传播的时候, 过长的序列导致梯度的计算异常, 发生梯度消失或爆炸.

- 什么是梯度消失或爆炸呢？

    根据反向传播算法和链式法则, 梯度的计算可以简化为以下公式:
    $$
    D_n = \sigma'(z_1)w_1 \cdot \sigma'(z_2)w_2 \cdot \dots \cdot \sigma'(z_n)w_n
    $$
    其中 sigmoid 的导数值域是固定的, 在 $[0, 0.25]$ 之间, 而一旦公式中的 $w$ 也小于 1, 那么通过这样的公式连乘后, 最终的梯度就会变得非常非常小, 这种现象称作梯度消失。反之, 如果我们人为的增大 $w$ 的值, 使其大于 1, 那么连乘够就可能造成梯度过大, 称作梯度爆炸.

- 梯度消失或爆炸的危害:

    如果在训练过程中发生了梯度消失, 权重无法被更新, 最终导致训练失败；梯度爆炸所带来的梯度过大, 大幅度更新网络参数, 在极端情况下, 结果会溢出 ($NaN$ 值).

## 1.3 LSTM 模型

- LSTM (Long Short-Term Memory) 也称长短时记忆结构, 它是传统 RNN 的变体, 与经典 RNN 相比能够有效捕捉长序列之间的语义关联, 缓解梯度消失或爆炸现象. 同时 LSTM 的结构更复杂, 它的核心结构可以分为四个部分去解析:
    - 遗忘门
    - 输入门
    - 细胞状态
    - 输出门

- LSTM 的内部结构图:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/31.png" alt="img" style="zoom: 67%;" />

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/22.png" alt="img" style="zoom:80%;" />

- 遗忘门部分结构图与计算公式:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/32.png" alt="img" style="zoom: 80%;" />

- 遗忘门结构分析:
  
    与传统 RNN 的内部结构计算非常相似, 首先将当前时间步输入 $x_t$ 与上一个时间步隐含状态 $h_{t-1}$ 拼接, 得到 $[x_t, h_{t-1}]$, 然后通过一个全连接层做变换, 最后通过 $Sigmoid$ 函数进行激活得到 $f_t$, 我们可以将 $f_t$ 看作是门值, 好比一扇门开合的大小程度, 门值都将作用在通过该扇门的张量, 遗忘门门值将作用的上一层的细胞状态上, 代表遗忘过去的多少信息, 又因为遗忘门门值是由 $x_t$, $h_{t-1}$ 计算得来的, 因此整个公式意味着根据当前时间步输入和上一个时间步隐含状态 $h_{t-1}$ 来决定遗忘多少上一层的细胞状态所携带的过往信息.

- 遗忘门内部结构过程演示:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN26.gif" alt="RNN26" style="zoom:80%;" />

- 激活函数 $Sigmiod$ 的作用:
  
    用于帮助调节流经网络的值, $Sigmoid$ 函数将值压缩在 0 和 1 之间.

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN27.gif" alt="RNN27" style="zoom:67%;" />

---

- 输入门部分结构图与计算公式:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/34.png" alt="img" style="zoom:80%;" />

- 输入门结构分析:
    - 我们看到输入门的计算公式有两个, 第一个就是产生输入门门值的公式, 它和遗忘门公式几乎相同, 区别只是在于它们之后要作用的目标上. 这个公式意味着输入信息有多少需要进行过滤。输入门的第二个公式是与传统 RNN 的内部结构计算相同. 对于 LSTM 来讲, 它得到的是当前的细胞状态, 而不是像经典 RNN 一样得到的是隐含状态.

- 输入门内部结构过程演示:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN28.gif" alt="RNN28" style="zoom:80%;" />

---

- 细胞状态更新图与计算公式:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/35.png" alt="img" style="zoom:80%;" />

- 细胞状态更新分析:
  
    细胞更新的结构与计算公式非常容易理解, 这里没有全连接层, 只是将刚刚得到的遗忘门门值与上一个时间步得到的 $C_{t-1}$ 相乘, 再加上输入门门值与当前时间步得到的未更新 $C_t$ 相乘的结果。最终得到更新后的 $C_t$ 作为下一个时间步输入的一部分. 整个细胞状态更新过程就是对遗忘门和输入门的应用.

- 细胞状态更新过程演示:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN29.gif" alt="RNN29" style="zoom: 80%;" />

---

- 输出门部分结构图与计算公式:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/37.png" alt="img" style="zoom:80%;" />

- 输出门结构分析:
  
    输出门部分的公式也是两个, 第一个即是计算输出门的门值, 它和遗忘门, 输入门计算方式相同. 第二个即是使用这个门值产生隐含状态 $h_t$, 他将作用在更新后的细胞状态 $C_t$ 上, 并做 $tanh$ 激活, 最终得到 $h_t$ 作为下一时间步输入的一部分. 整个输出门的过程, 就是为了产生隐含状态 $h_t$.

- 输出门内部结构过程演示:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/RNN30.gif" alt="RNN30" style="zoom:80%;" />

---

- 什么是 Bi-LSTM ?
  
    Bi-LSTM 即双向 LSTM, 它没有改变 LSTM 本身任何的内部结构, 只是将 LSTM 应用两次且方向不同, 再将两次得到的 LSTM 结果进行拼接作为最终输出.

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/38.png" alt="img" style="zoom: 67%;" />

- Bi-LSTM 结构分析:
  
    我们看到图中对 "我爱中国" 这句话或者叫这个输入序列, 进行了从左到右和从右到左两次 LSTM 处理, 将得到的结果张量进行了拼接作为最终输出. 这种结构能够捕捉语言语法中一些特定的前置或后置特征, 增强语义关联, 但是模型参数和计算复杂度也随之增加了一倍, 一般需要对语料和计算资源进行评估后决定是否使用该结构.

------

- Pytorch 中 LSTM 工具的使用:

    位置: 在 `torch.nn` 工具包之中, 通过 `torch.nn.LSTM` 可调用.

- `nn.LSTM` 类初始化主要参数解释:

    - `input_size`: 输入张量 $x$ 中特征维度的大小.

    - `hidden_size`: 隐层张量 $h$ 中特征维度的大小.

    - `num_layers`: 隐含层的数量.

    - `nonlinearity`: 激活函数的选择, 默认是 $tanh$.

    - `bidirectional`: 是否选择使用双向 LSTM, 如果为 True, 则使用; 默认不使用.

- `nn.LSTM` 类实例化对象主要参数解释:
    - `input`: 输入张量 $x$.
    - `h0`: 初始化的隐层张量 $h$.
    - `c0`: 初始化的细胞状态张量 $c$.

- `nn.LSTM` 使用示例:

```python
import torch.nn as nn
import torch

# 实例化LSTM对象
# input_size: 输入张量x的维度
# hidden_size: 隐藏层的维度，隐藏层神经元数量
# num_layers: 隐藏层的层数
lstm = nn.LSTM(5, 6, 2)

# 初始化输入张量x
# sequence_length: 输入序列的长度
# batch_size: 批次的样本数
# input_size: 输入张量x的维度
x = torch.randn(1, 3, 5)

# 初始化隐藏层张量h0, 和细胞状态c0
# num_layers * num_directions: 隐藏层的层数*网络方向数
# batch_size: 批次的样本数
# hidden_size: 隐藏层的维度
h0 = torch.randn(2, 3, 6)
c0 = torch.randn(2, 3, 6)

# 将x, h0, c0输入lstm中, 得到输出张量结果
output, (hn, cn) = lstm(x, (h0, c0))

print(output)
print(output.shape)
print(hn)
print(hn.shape)
print(cn)
print(cn.shape)
```

```python
tensor([[[-0.1336, -0.1749,  0.1248, -0.3440,  0.2934, -0.2653],
         [ 0.0353,  0.0299,  0.2004,  0.1482,  0.1328, -0.1831],
         [ 0.1052,  0.3167,  0.0142,  0.1864, -0.0552,  0.0805]]],
       grad_fn=<StackBackward>)
torch.Size([1, 3, 6])
tensor([[[-0.2178,  0.5074, -0.1410, -0.0276,  0.1927, -0.3116],
         [ 0.0961,  0.3443, -0.0842, -0.0764,  0.3239,  0.1411],
         [-0.3050,  0.0396,  0.0110,  0.1763,  0.4789,  0.2153]],

        [[-0.1336, -0.1749,  0.1248, -0.3440,  0.2934, -0.2653],
         [ 0.0353,  0.0299,  0.2004,  0.1482,  0.1328, -0.1831],
         [ 0.1052,  0.3167,  0.0142,  0.1864, -0.0552,  0.0805]]],
       grad_fn=<StackBackward>)
torch.Size([2, 3, 6])
tensor([[[-0.5326,  0.9175, -0.4234, -0.0604,  0.6048, -0.4205],
         [ 0.2847,  1.0213, -0.3338, -0.1736,  0.5604,  0.4445],
         [-0.4224,  0.1378,  0.0485,  0.2175,  0.7337,  0.8314]],

        [[-0.4219, -0.6792,  0.2880, -1.3118,  0.6268, -0.5990],
         [ 0.0950,  0.0434,  0.6786,  0.5461,  0.3989, -0.4596],
         [ 0.2458,  0.7993,  0.0486,  0.5484, -0.1500,  0.1786]]],
       grad_fn=<StackBackward>)
torch.Size([2, 3, 6])
```

- LSTM 优势:

    LSTM 的门结构能够有效减缓长序列问题中可能出现的梯度消失或爆炸, 虽然并不能杜绝这种现象, 但在更长的序列问题上表现优于传统 RNN.

- LSTM 缺点:

    由于内部结构相对较复杂, 因此训练效率在同等算力下较传统 RNN 低很多.

## 1.4 GRU 模型

- GRU (Gated Recurrent Unit) 也称门控循环单元结构, 它也是传统 RNN 的变体, 同 LSTM 一样能够有效捕捉长序列之间的语义关联, 缓解梯度消失或爆炸现象. 同时它的结构和计算要比 LSTM 更简单, 它的核心结构可以分为两个部分去解析:
    - 更新门
    - 重置门

- GRU 的内部结构图和计算公式:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/gru.png" alt="gru" style="zoom:80%;" />

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/22.png" alt="img" style="zoom: 80%;" />

- GRU 的更新门和重置门结构图:

<img src="1.RNN%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90.assets/gru2.png" alt="gru2" style="zoom: 67%;" />

- 内部结构分析:
  
    和之前分析过的 LSTM 中的门控一样, 首先计算更新门和重置门的门值, 分别是 $z_t$ 和 $r_t$, 计算方法就是使用 $X_t$ 与 $h_{t-1}$ 拼接进行线性变换, 再经过 $Sigmoid$ 激活. 之后更新门门值作用在了 $h_{t-1}$ 上, 代表控制上一时间步传来的信息有多少可以被利用. 接着就是使用这个更新后的 $h_{t-1}$ 进行基本的 RNN 计算, 即与 $x_t$ 拼接进行线性变化, 经过 $tanh$ 激活, 得到新的 $h_t$. 最后重置门的门值会作用在新的 $h_t$, 而 $1-z_t$ 会作用在 $h_{t-1}$ 上, 随后将两者的结果相加, 得到最终的隐含状态输出 $h_t$, 这个过程意味着重置门有能力重置之前所有的计算, 当门值趋于 1 时, 输出就是新的 $h_t$, 而当门值趋于 0 时, 输出就是上一时间步的 $h_{t-1}$.

- Bi-GRU 与 Bi-LSTM 的逻辑相同, 都是不改变其内部结构, 而是将模型应用两次且方向不同, 再将两次得到的 LSTM 结果进行拼接作为最终输出。具体参见上小节中的 Bi-LSTM.

---

- Pytorch 中 GRU 工具的使用:
  
    位置：在 `torch.nn` 工具包之中, 通过 `torch.nn.GRU` 可调用.

- `nn.GRU` 类初始化主要参数解释:
  
    - `input_size`: 输入张量 $x$ 中特征维度的大小.
    - `hidden_size`: 隐层张量 $h$ 中特征维度的大小.
    - `num_layers`: 隐含层的数量.
    - `nonlinearity`: 激活函数的选择, 默认是 $tanh$.
    - `bidirectional`: 是否选择使用双向 LSTM, 如果为 True, 则使用; 默认不使用.

- `nn.GRU` 类实例化对象主要参数解释:
    - `input`: 输入张量 $x$.
    - `h0`: 初始化的隐层张量 $h$.

- `nn.GRU` 使用示例:

```python
import torch
import torch.nn as nn

# 实例化GRU对象
# input_size: 输入张量x的维度
# hidden_size: 隐藏层的维度，隐藏层神经元数量
# num_layers: 隐藏层的层数
gru = nn.GRU(5, 6, 2)

# 初始化输入张量x
# sequence_length: 输入序列的长度
# batch_size: 批次的样本数
# input_size: 输入张量x的维度
x = torch.randn(1, 3, 5)

# 初始化隐藏层张量h0
# num_layers * num_directions: 隐藏层的层数*网络方向数
# batch_size: 批次的样本数
# hidden_size: 隐藏层的维度
h0 = torch.randn(2, 3, 6)

# 将x, h0输入gru中, 得到输出张量结果
output, hn = gru(x, h0)

print(output)
print(output.shape)
print(hn)
print(hn.shape)
```

```python
tensor([[[-0.0736, -0.2910, -0.2255, -0.1893, -0.1557,  0.5144],
         [ 0.9000, -0.1368,  0.0599,  0.1087,  0.1486,  0.2793],
         [ 0.3098,  0.1956,  0.1551, -0.7526, -0.6638,  0.0019]]],
       grad_fn=<StackBackward>)
torch.Size([1, 3, 6])
tensor([[[ 0.3531, -1.1666, -0.6895,  0.4093, -0.2552,  0.0722],
         [ 0.0638,  1.2192,  0.1624,  0.5599, -0.1000,  0.0241],
         [ 0.4553,  0.1781,  0.1459,  0.6360,  0.1615, -0.4576]],

        [[-0.0736, -0.2910, -0.2255, -0.1893, -0.1557,  0.5144],
         [ 0.9000, -0.1368,  0.0599,  0.1087,  0.1486,  0.2793],
         [ 0.3098,  0.1956,  0.1551, -0.7526, -0.6638,  0.0019]]],
       grad_fn=<StackBackward>)
torch.Size([2, 3, 6])
```

- GRU 的优势:
    - GRU 和 LSTM 作用相同, 在捕捉长序列语义关联时, 能有效抑制梯度消失或爆炸, 效果都优于传统 RNN 且计算复杂度相比 LSTM 要小.

- GRU 的缺点:
    - GRU 仍然不能完全解决梯度消失问题, 同时其作用 RNN 的变体, 有着 RNN 结构本身的一大弊端, 即不可并行计算, 这在数据量和模型体量逐步增大的未来, 是 RNN 发展的关键瓶颈.

## 1.5 注意力机制

- 什么是注意力:
  
    我们观察事物时, 之所以能够快速判断一种事物 (当然允许判断是错误的), 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断, 而并非是从头到尾的观察一遍事物后, 才能有判断结果. 正是基于这样的理论, 就产生了注意力机制.

- 什么是注意力计算规则:
  
    它需要三个指定的输入 $Q(query), K(key), V(value)$, 然后通过计算公式得到注意力的结果, 这个结果代表 $query$ 在 $key$ 和 $value$ 作用下的注意力表示. 当输入的 $Q=K=V$ 时, 称作自注意力计算规则.

- 常见的注意力计算规则:

    将 $Q, K$ 进行纵轴拼接, 做一次线性变化, 再使用 $Softmax$ 处理获得结果最后与 $V$ 做张量乘法.
    $$
    Attention(Q,K,V) = Softmax(Linear([Q,K])) \cdot V
    $$
    

    将 $Q, K$ 进行纵轴拼接, 做一次线性变化后再使用 $tanh$ 函数激活, 然后再进行内部求和, 最后使用 $Softmax$ 处理获得结果再与 $V$ 做张量乘法.
    $$
    Attention(Q,K,V) = Softmax(sum(tanh(Linear([Q,K])))) \cdot V
    $$
    

    将 $Q$ 与 $K$ 的转置做点积运算, 然后除以一个缩放系数, 再使用 $Softmax$ 处理获得结果最后与 $V$ 做张量乘法.
    $$
    Attention(Q,K,V) = Softmax(\frac{Q \cdot K^T}{\sqrt{d_k}}) \cdot V
    $$

> 说明：当注意力权重矩阵和 $V$ 都是三维张量且第一维代表为 batch 条数时, 则做 `bmm` 运算. `bmm` 是一种特殊的张量乘法运算.

- `bmm` 运算演示:

```python
# 如果参数1形状是(b × n × m), 参数2形状是(b × m × p), 则输出为(b × n × p)
mat1 = torch.randn(10, 3, 4)
mat2 = torch.randn(10, 4, 5)
res = torch.bmm(mat1, mat2)
print(res.size())
```

```python
torch.Size([10, 3, 5])
```

- 什么是注意力机制:

    注意力机制是注意力计算规则能够应用的深度学习网络的载体, 同时包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体. 使自注意力计算规则的注意力机制称为自注意力机制.

> 说明: NLP 领域中, 当前的注意力机制大多数应用与 seq2seq 架构, 即编码器和解码器模型.

- 注意力机制的作用:

    1. 在解码器端的注意力机制: 能够根据模型目标有效的聚焦编码器的输出结果, 当其作为解码器的输入时提升效果. 改善以往编码器输出是单一定长张量, 无法存储过多信息的情况.

    2. 在编码器端的注意力机制: 主要解决表征问题, 相当于特征提取过程, 得到输入的注意力表示. 一般使用自注意力 (self-attention).
- 注意力机制实现步骤:

    1. 根据注意力计算规则, 对 $Q, K, V$ 进行相应的计算.

    2. 根据第一步采用的计算方法, 如果是拼接方法, 则需要将 $Q$ 与第二步的计算结果再进行拼接, 如果是转置点积, 一般是自注意力, $Q$ 与 $V$ 相同, 则不需要进行与 $Q$ 的拼接.

    3. 最后为了使整个 attention 机制按照指定尺寸输出, 使用线性层作用在第二步的结果上做一个线性变换, 得到最终对 $Q$ 的注意力表示.
- 常见注意力机制的代码分析:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Attn(nn.Module):
    def __init__(self, query_size, key_size, value_size1, value_size2, output_size):
        """V -> (1, value_size1, value_size2)
           query_size:	Q的最后一维大小
           key_size:	K的最后一维大小
           value_size1:	V的倒数第二维大小
           value_size2:	V的倒数第一维大小,
           output_size:	输出的最后一维大小"""
        super(Attn, self).__init__()
        # 将以下参数传入类中
        self.query_size = query_size
        self.key_size = key_size
        self.value_size1 = value_size1
        self.value_size2 = value_size2
        self.output_size = output_size

        # 初始化注意力机制实现第一步中需要的线性层.
        self.attn = nn.Linear(self.query_size + self.key_size, value_size1)

        # 初始化注意力机制实现第三步中需要的线性层.
        self.attn_combine = nn.Linear(self.query_size + value_size2, output_size)


    def forward(self, Q, K, V):
        """注意我们假定Q, K, V都是三维张量"""

        # 我们采用常见的第一种计算规则
        # 第一步，将Q, K进行纵轴拼接, 做一次线性变化, 最后使用softmax进行处理得到注意力向量
        attn_weights = F.softmax(
            self.attn(torch.cat((Q[0], K[0]), 1)), dim=1)

        # 将注意力矩阵和V进行一次bmm运算
        attn_applied = torch.bmm(attn_weights.unsqueeze(0), V)

        # 再次取[0]进行降维, 将Q与第一步的计算结果再进行拼接
        output = torch.cat((Q[0], attn_applied[0]), 1)

        # 最后是第三步, 使用线性层作用在第三步的结果上做一个线性变换并扩展维度, 得到输出
        output = self.attn_combine(output).unsqueeze(0)
        return output, attn_weights
```

- 调用:

```python
query_size = 32
key_size = 32
value_size1 = 32
value_size2 = 64
output_size = 64

attn = Attn(query_size, key_size, value_size1, value_size2, output_size)
Q = torch.randn(1,1,32)
K = torch.randn(1,1,32)
V = torch.randn(1,32,64)
out = attn(Q, K ,V)
print(out[0])
print(out[0].size())
print(out[1])
print(out[1].size())
```

- 输出效果:

```python
tensor([[[-0.1380,  0.0446,  0.3892, -0.2732,  0.1141, -0.1469, -0.1686,
          -0.3764,  0.2442,  0.5459, -0.0138,  0.3068,  0.2234, -0.6009,
           0.3197, -0.6230, -0.6360, -1.0510, -0.6008,  0.3104, -0.1392,
          -0.3098,  0.2633,  0.0690, -0.2519,  0.2432,  0.3531, -0.0299,
           0.1910,  0.6355, -0.6648,  0.2458,  0.0300,  0.1650, -0.4909,
          -0.0250, -0.6257, -0.3863, -0.5463, -0.2629,  0.0395,  0.5395,
           0.6417, -0.5023, -0.4886,  0.1039,  0.0789,  0.4373,  0.2934,
           0.1173, -0.2905, -0.0032,  0.8135,  0.2768,  0.0210,  0.0737,
          -0.0954,  0.1633, -0.1256, -0.7121,  0.1158, -0.2069,  1.4607,
           0.2850]]], grad_fn=<UnsqueezeBackward0>)
torch.Size([1, 1, 64])
tensor([[0.0551, 0.0368, 0.0087, 0.0285, 0.0266, 0.0131, 0.0401, 0.0327, 0.0187,
         0.0208, 0.0146, 0.0565, 0.0181, 0.0442, 0.0367, 0.0253, 0.0350, 0.0268,
         0.0715, 0.0279, 0.0518, 0.0249, 0.0365, 0.0194, 0.0180, 0.0198, 0.0225,
         0.0318, 0.0213, 0.0456, 0.0619, 0.0090]], grad_fn=<SoftmaxBackward>)
torch.Size([1, 32])
```
