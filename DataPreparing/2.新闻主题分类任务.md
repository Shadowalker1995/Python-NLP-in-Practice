# 2. 新闻主题分类任务

## 2.1 新闻主题分类任务

- 关于新闻主题分类任务:

    以一段新闻报道中的文本描述内容为输入, 使用模型帮助我们判断它最有可能属于哪一种类型的新闻, 这是典型的文本分类问题, 我们这里假定每种类型是互斥的, 即文本描述有且只有一种类型.

- 通过 torchtext 获取新闻主题分类数据:

```python
import torch
import torchtext
# 导入torchtext.datasets中的文本分类任务
from torchtext.datasets import text_classification
import os

load_data_path = "./data/"
if not os.path.isdir(load_data_path):
    os.mkdir(load_data_path)

# 选取torchtext中的文本分类数据集'AG_NEWS'即新闻主题分类数据, 保存在指定目录下
# 并将数值映射后的训练和验证数据加载到内存中
train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](root=load_data_path)
```

```bash
120000lines [00:14, 8115.22lines/s]
120000lines [00:27, 4418.27lines/s]
7600lines [00:01, 4013.67lines/s]
```

- 数据文件预览:

```
- data/
    - ag_news_csv.tar.gz
    - ag_news_csv/
        classes.txt
        readme.txt
        test.csv
        train.csv
```

- 文件说明:

    `train.csv`表示训练数据, 共 12 万条数据

    `test.csv`表示验证数据, 共 7600 条数据

    `classes.txt`是标签 (新闻主题) 含义文件, 里面有四个单词'World', 'Sports', 'Business', 'Sci/Tech' 代表新闻的四个主题

    `readme.txt`是该数据集的英文说明

- `train.csv`预览:

```
"3","Wall St. Bears Claw Back Into the Black (Reuters)","Reuters - Short-sellers, Wall Street's dwindling\band of ultra-cynics, are seeing green again."
"3","Carlyle Looks Toward Commercial Aerospace (Reuters)","Reuters - Private investment firm Carlyle Group,\which has a reputation for making well-timed and occasionally\controversial plays in the defense industry, has quietly placed\its bets on another part of the market."
"3","Oil and Economy Cloud Stocks' Outlook (Reuters)","Reuters - Soaring crude prices plus worries\about the economy and the outlook for earnings are expected to\hang over the stock market next week during the depth of the\summer doldrums."
"3","Iraq Halts Oil Exports from Main Southern Pipeline (Reuters)","Reuters - Authorities have halted oil export\flows from the main pipeline in southern Iraq after\intelligence showed a rebel militia could strike\infrastructure, an oil official said on Saturday."
"3","Oil prices soar to all-time record, posing new menace to US economy (AFP)","AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections."
"3","Stocks End Up, But Near Year Lows (Reuters)","Reuters - Stocks ended slightly higher on Friday\but stayed near lows for the year as oil prices surged past  #36;46\a barrel, offsetting a positive outlook from computer maker\Dell Inc. (DELL.O)"
"3","Money Funds Fell in Latest Week (AP)","AP - Assets of the nation's retail money market mutual funds fell by  #36;1.17 billion in the latest week to  #36;849.98 trillion, the Investment Company Institute said Thursday."
"3","Fed minutes show dissent over inflation (USATODAY.com)","USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump."
"3","Safety Net (Forbes.com)","Forbes.com - After earning a PH.D. in Sociology, Danny Bazil Riley started to work as the general manager at a commercial real estate firm at an annual base salary of  #36;70,000. Soon after, a financial planner stopped by his desk to drop off brochures about insurance benefits available through his employer. But, at 32, ""buying insurance was the furthest thing from my mind,"" says Riley."
"3","Wall St. Bears Claw Back Into the Black"," NEW YORK (Reuters) - Short-sellers, Wall Street's dwindling  band of ultra-cynics, are seeing green again."
```

- 文件内容说明:

    `train.csv` 共由 3 列组成, 使用 ',' 进行分隔, 分别代表: 标签, 新闻标题, 新闻简述

    其中标签用 "1", "2", "3", "4" 表示, 依次对应 classes 中的内容.

- test.csv 与 train.csv 内容格式与含义相同.

**整个案例的实现可分为以下五个步骤**

1. 构建带有 Embedding 层的文本分类模型
2. 对数据进行 batch 处理
3. 构建训练与验证函数
4. 进行模型训练和验证
5. 查看 embedding 层嵌入的词向量

1. **构建带有 Embedding 层的文本分类模型**

```python
import torch.nn as nn
import torch.nn.functional as F

BATCH_SIZE = 16
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class TextSentiment(nn.Module):
    """文本分类模型"""
    def __init__(self, vocab_size, embed_dim, num_class):
        """
        :param vocab_size: 代表整个语料包含的单词总数
        :param embed_dim:  代表词嵌入的维度
        :param num_class:  代表是文本分类的类别数
        """
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.fc = nn.Linear(embed_dim, num_class)
        self.init_weights()

    def init_weights(self):
        """初始化权重函数"""
        # 指定初始权重的取值范围数
        initrange = 0.5
        # 各层的权重使用均匀分布进行初始化
        self.embedding.weight.data.uniform_(-initrange, initrange)
        self.fc.weight.data.uniform_(-initrange, initrange)
        # 偏置初始化为0
        self.fc.bias.data.zero_()

    def forward(self, text):
        """
        :param text: 文本经数字化映射后的张量

        :return: 与类别数尺寸相同的张量, 用以判断文本类别
        """
        # m * 32, 其中m是BATCH_SIZE大小的数据中词汇总数
        embedded = self.embedding(text)
        # 将 m * 32 转化成 BATCH_SIZE * 32, 以便通过fc层后能计算相应的损失
        # 已知m的值远大于BATCH_SIZE, 用m整除BATCH_SIZE
        # 获得m中共包含c个BATCH_SIZE
        c = embedded.size(0) // BATCH_SIZE
        # 从embedded中取 BATCH_SIZE*c 个向量得到新的embedded
        # 新的embedded中的向量个数可以整除BATCH_SIZE
        embedded = embedded[:BATCH_SIZE*c]
        # 为了利用平均池化的方法求embedded中指定行数的列的平均数
        # 但平均池化方法是作用在行上的, 并且需要3维输入
        # 因此对新的embedded进行转置并拓展维度
        embedded = embedded.transpose(1, 0).unsqueeze(0)
        # 调用平均池化的方法, 并且核的大小为c, 即取每c的元素计算一次均值作为结果
        embedded = F.avg_pool1d(embedded, kernel_size=c)
        # 消除新增的维度, 然后转置回去输送给fc层
        return self.fc(embedded[0].transpose(1, 0))
```

- 实例化模型:

```python
# 获得整个语料包含的不同词汇总数
VOCAB_SIZE = len(train_dataset.get_vocab())
# 指定词嵌入维度
EMBED_DIM = 32
# 获得类别总数
NUM_CLASS = len(train_dataset.get_labels())
# 实例化模型
model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)
```

2. **对数据进行 batch 处理**

```python
def generate_batch(batch):
    """
    生成batch数据函数
    :param batch: 由样本张量和对应标签的元组组成的batch_size大小的列表
                  形如:
                  [(label1, sample1), (lable2, sample2), ..., (labelN, sampleN)]

    return: 样本张量和标签各自的列表形式(张量)
            形如:
            text = tensor([sample1, sample2, ..., sampleN])
            label = tensor([label1, label2, ..., labelN])
    """
    # 从batch中获得标签张量
    label = torch.tensor([entry[0] for entry in batch])
    # 从batch中获得样本张量
    text = [entry[1] for entry in batch]
    text = torch.cat(text)
    return text, label
```

- 调用:

```python
# 假设一个输入:
batch = [(1, torch.tensor([3, 23, 2, 8])), (0, torch.tensor([3, 45, 21, 6]))]
res = generate_batch(batch)
print(res)
```

- 输出效果:

```python
# 对应输入的两条数据进行了相应的拼接
(tensor([ 3, 23,  2,  8,  3, 45, 21,  6]), tensor([1, 0]))
```

3. **构建训练与验证函数**

```python
# 导入torch中的数据加载器方法
from torch.utils.data import DataLoader

def train(train_data):
    """模型训练函数"""
    # 初始化训练损失和准确率为0
    train_loss = 0
    train_acc = 0

    # 使用数据加载器构建批次数据
    # 使用数据加载器生成BATCH_SIZE大小的数据进行批次训练
    # data为N多个generate_batch函数处理后的BATCH_SIZE大小的数据生成器
    data = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)

    # 对data进行循环遍历, 使用每个batch数据进行参数更新
    for i, (text, cls) in enumerate(data):
        # 设置优化器初始梯度为0
        optimizer.zero_grad()
        # 模型输入一个批次数据, 获得输出
        output = model(text)
        # 根据真实标签与模型输出计算损失
        try:
            loss = criterion(output, cls)
        except:
            continue
        # 将该批次的损失值累加到总损失中
        train_loss += loss.item()
        # 误差反向传播
        loss.backward()
        # 参数更新
        optimizer.step()
        # 计算该批次的准确率
        batch_acc = (output.argmax(1) == cls).sum().item()
        # 将该批次的准确率加到总准确率中
        train_acc += batch_acc
        if (i + 1) % 100 == 0:
            print('Batch {}, Acc: {}'.format(i + 1, 1.0 * batch_acc / BATCH_SIZE))

    # 调整优化器学习率
    scheduler.step()

    # 返回本轮训练的平均损失和平均准确率
    return train_loss / len(train_data), train_acc / len(train_data)

def valid(valid_data):
    """模型验证函数"""
    # 初始化验证损失和准确率为0
    valid_loss = 0
    valid_acc = 0

    # 和训练相同, 使用DataLoader获得训练数据生成器
    data = DataLoader(valid_data, batch_size=BATCH_SIZE, collate_fn=generate_batch)

    # 按批次取出数据验证
    for text, cls in data:
        # 验证阶段, 不再求解梯度
        with torch.no_grad():
            # 使用模型获得输出
            output = model(text)
            # 计算损失
            try:
                loss = criterion(output, cls)
            except:
                continue
            # 将损失和准确率加到总损失和准确率
            valid_loss += loss.item()
            valid_acc += (output.argmax(1) == cls).sum().item()

    # 返回本轮验证的平均损失和平均准确率
    return valid_loss / len(valid_data), valid_acc / len(valid_data)
```

4. **进行模型训练和验证**

```python
# 导入时间工具包
import time
# 导入数据随机划分方法工具
from torch.utils.data.dataset import random_split

# 指定训练轮数
N_EPOCHS = 20
# 定义初始的验证损失
min_valid_loss = float('inf')

# 选择预定义的交叉熵损失函数
criterion = torch.nn.CrossEntropyLoss().to(device)
# 选择随机梯度下降优化器
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
# 选择优化器步长调节方法StepLR, 用来衰减学习率
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)

# 选择全部训练数据的95%作为训练集数据, 剩下的5%作为验证数据
train_len = int(len(train_dataset) * 0.95)
# print('train_len:', train_len)
# print('valid_len:', len(train_dataset) - train_len)
# 使用random_split进行乱序划分, 得到对应的训练集和验证集
sub_train_, sub_valid_ = random_split(train_dataset, [train_len, len(train_dataset) - train_len])

# 设定模型保存的路径
MODEL_PATH = './models/news_model.pth'
if not os.path.exists(MODEL_PATH):
    # 开始每一轮训练
    for epoch in range(N_EPOCHS):
        # 记录训练开始的时间
        start_time = time.time()
        # 调用train和valid函数得到训练和验证的平均损失, 平均准确率
        train_loss, train_acc = train(sub_train_)
        valid_loss, valid_acc = valid(sub_valid_)

        # 模型保存
        torch.save(model.state_dict(), MODEL_PATH)
        print('The model saved epoch {}'.format(epoch))

        # 计算训练和验证的总耗时(秒)
        secs = int(time.time() - start_time)
        # 用分钟和秒表示
        mins = secs / 60
        secs = secs % 60

        # 打印训练和验证耗时, 平均损失, 平均准确率
        print('Epoch: %d' % (epoch + 1), " | time in %d minites, %d seconds" % (mins, secs))
        print(f'\tLoss: {train_loss:.4f}(train)\t|\tAcc: {train_acc * 100:.1f}%(train)')
        print(f'\tLoss: {valid_loss:.4f}(valid)\t|\tAcc: {valid_acc * 100:.1f}%(valid)')
else:
    # 如果未来要重新加载模型, 实例化model后执行以下命令
    model.load_state_dict(torch.load(MODEL_PATH))
```

- 输出效果:

```shell
120000lines [00:11, 10019.40lines/s]
120000lines [00:19, 6025.34lines/s]
7600lines [00:01, 4675.46lines/s]
train_len: 114000
valid_len: 6000
Epoch: 1  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 27.0%(train)
        Loss: 0.0025(valid)     |       Acc: 27.9%(valid)
Epoch: 2  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 30.2%(train)
        Loss: 0.0025(valid)     |       Acc: 29.6%(valid)
Epoch: 3  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 32.4%(train)
        Loss: 0.0025(valid)     |       Acc: 30.6%(valid)
Epoch: 4  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 32.7%(train)
        Loss: 0.0025(valid)     |       Acc: 31.4%(valid)
Epoch: 5  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 32.9%(train)
        Loss: 0.0025(valid)     |       Acc: 31.6%(valid)
Epoch: 6  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 33.1%(train)
        Loss: 0.0025(valid)     |       Acc: 31.9%(valid)
Epoch: 7  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 33.3%(train)
        Loss: 0.0025(valid)     |       Acc: 31.9%(valid)
Epoch: 8  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 32.5%(train)
        Loss: 0.0025(valid)     |       Acc: 32.0%(valid)
Epoch: 9  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 33.7%(train)
        Loss: 0.0025(valid)     |       Acc: 32.2%(valid)
Epoch: 10  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 33.6%(train)
        Loss: 0.0025(valid)     |       Acc: 32.5%(valid)
Epoch: 11  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 32.8%(train)
        Loss: 0.0025(valid)     |       Acc: 32.7%(valid)
Epoch: 12  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 33.9%(train)
        Loss: 0.0025(valid)     |       Acc: 32.6%(valid)
Epoch: 13  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 33.8%(train)
        Loss: 0.0025(valid)     |       Acc: 32.6%(valid)
Epoch: 14  | time in 0 minites, 7 seconds
        Loss: 0.0026(train)     |       Acc: 34.1%(train)
        Loss: 0.0025(valid)     |       Acc: 32.7%(valid)
Epoch: 15  | time in 0 minites, 7 seconds
        Loss: 0.0026(train)     |       Acc: 34.3%(train)
        Loss: 0.0025(valid)     |       Acc: 32.6%(valid)
Epoch: 16  | time in 0 minites, 7 seconds
        Loss: 0.0026(train)     |       Acc: 34.7%(train)
        Loss: 0.0025(valid)     |       Acc: 32.7%(valid)
Epoch: 17  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 33.2%(train)
        Loss: 0.0025(valid)     |       Acc: 32.7%(valid)
Epoch: 18  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 34.0%(train)
        Loss: 0.0025(valid)     |       Acc: 32.7%(valid)
Epoch: 19  | time in 0 minites, 7 seconds
        Loss: 0.0027(train)     |       Acc: 33.7%(train)
        Loss: 0.0025(valid)     |       Acc: 32.6%(valid)
Epoch: 20  | time in 0 minites, 7 seconds
        Loss: 0.0026(train)     |       Acc: 34.2%(train)
        Loss: 0.0025(valid)     |       Acc: 32.5%(valid)
```

5. **查看 embedding 层嵌入的词向量**

```python
# 打印从模型的状态字典中获得的Embedding矩阵
print('********************')
print(model.state_dict()['embedding.weight'])
```

- 输出效果:

```python
tensor([[-0.0474, -0.1239, -0.0440,  ...,  0.1305,  0.3774, -0.3091],
        [-0.0336, -0.1538,  0.3855,  ..., -0.2622,  0.0866,  0.0653],
        [ 0.1400, -0.4347, -0.1900,  ...,  0.0924,  0.2777,  0.3888],
        ...,
        [-0.1330,  0.2010,  0.2931,  ...,  0.2826, -0.1040,  0.0917],
        [ 0.0918,  0.1489, -0.3626,  ...,  0.1697,  0.4534,  0.0328],
        [ 0.3372,  0.1231, -0.2216,  ...,  0.2609,  0.0271,  0.1261]],
       device='cuda:0')
```